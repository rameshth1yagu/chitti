ðŸ¤– [Chitti Watchdog] Starting System Hardening...
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
ðŸ”’ [Chitti Watchdog] Privacy-First AI Server Launching...
âœ… [Chitti Watchdog] System is Armed and Lean.
   -> CUDA VRAM Optimized
   -> Zero-Retention Policy Applied
ðŸ¤– [Chitti Watchdog] Starting System Hardening...
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
ðŸ”’ [Chitti Watchdog] Privacy-First AI Server Launching...
âœ… [Chitti Watchdog] System is Armed and Lean.
   -> CUDA VRAM Optimized
   -> Zero-Retention Policy Applied
ðŸ¤– [Chitti Watchdog] Starting System Hardening...
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
ðŸ”’ [Chitti Watchdog] Privacy-First AI Server Launching...
âœ… [Chitti Watchdog] System is Armed and Lean.
   -> CUDA VRAM Optimized
   -> Zero-Retention Policy Applied
ðŸ¤– [Chitti Watchdog] Starting System Hardening...
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
3
ðŸ”’ [Chitti Watchdog] Privacy-First AI Server Launching...
âœ… [Chitti Watchdog] System is Armed and Lean.
   -> CUDA VRAM Optimized
   -> Zero-Retention Policy Applied
ðŸ¤– [Chitti Watchdog] Starting System Hardening...
sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper
sudo: a password is required
3
ðŸ”’ [Chitti Watchdog] Privacy-First AI Server Launching...
âœ… [Chitti Watchdog] System is Armed and Lean.
   -> CUDA VRAM Optimized
   -> Zero-Retention Policy Applied
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-10T22:27:26.846-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-10T22:27:26.852-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-10T22:27:26.852-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-10T22:27:26.853-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-10T22:27:26.858-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-10T22:27:26.861-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46797"
time=2026-02-10T22:27:29.144-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42003"
time=2026-02-10T22:27:29.374-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="4.6 GiB"
time=2026-02-10T22:27:29.375-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-10T22:27:50.225-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44479"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:27:50.707-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:27:50.708-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 46479"
time=2026-02-10T22:27:50.709-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:27:50.709-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:27:50.709-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:27:50.710-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:27:50.710-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:27:50.710-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:27:50.710-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:27:50.730-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:27:50.799-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:27:50.806-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:46479"
time=2026-02-10T22:27:50.808-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:27:50.809-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:27:50.810-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4737692
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4626 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:28:14.076-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.37 seconds"
time=2026-02-10T22:28:14.077-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:28:14.077-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:28:14.078-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.37 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:28:16 | 200 | 26.291789737s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:28:29.879-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42715"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:28:31.358-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:28:31.358-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 37297"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:28:31.359-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:28:31.359-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:28:31.380-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:28:31.450-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:28:31.456-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:37297"
time=2026-02-10T22:28:31.459-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:28:31.459-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:28:31.460-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4730280
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4619 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:28:33.723-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.36 seconds"
time=2026-02-10T22:28:33.724-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:28:33.724-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:28:33.725-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.37 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:28:35 | 200 |   5.80863738s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:28:43.078-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37149"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:28:43.565-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:28:43.566-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 36485"
time=2026-02-10T22:28:43.566-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:28:43.566-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:28:43.566-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:28:43.567-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:28:43.567-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:28:43.567-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:28:43.567-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:28:43.587-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:28:43.655-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:28:43.660-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:36485"
time=2026-02-10T22:28:43.667-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:28:43.668-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:28:43.668-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4697344
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4587 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:28:47.189-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.62 seconds"
time=2026-02-10T22:28:47.189-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:28:47.189-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:28:47.190-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.62 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:28:48 | 200 |  5.973731243s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:32:19.543-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46537"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:32:19.983-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:32:19.984-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 45587"
time=2026-02-10T22:32:19.984-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:32:19.985-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:32:19.985-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:32:19.985-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:32:19.985-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:32:19.985-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:32:19.985-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:32:20.005-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:32:20.077-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:32:20.083-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:45587"
time=2026-02-10T22:32:20.095-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:32:20.096-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:32:20.096-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4750292
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4638 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:32:23.871-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.89 seconds"
time=2026-02-10T22:32:23.872-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:32:23.872-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:32:23.872-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.89 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:32:25 | 200 |  5.842906071s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:32:43.494-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45807"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:32:43.961-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:32:43.961-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 42665"
time=2026-02-10T22:32:43.962-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:32:43.962-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:32:43.962-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:32:43.963-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:32:43.963-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:32:43.963-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:32:43.963-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:32:43.984-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:32:44.056-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:32:44.062-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:42665"
time=2026-02-10T22:32:44.072-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:32:44.072-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:32:44.073-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4740312
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4629 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:32:47.343-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.38 seconds"
time=2026-02-10T22:32:47.343-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:32:47.343-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:32:47.344-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.38 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:32:48 | 200 |  5.362311134s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:34:10.702-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46045"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:34:11.174-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:34:11.175-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 42423"
time=2026-02-10T22:34:11.176-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:34:11.176-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:34:11.176-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:34:11.177-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:34:11.177-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:34:11.177-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:34:11.177-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:34:11.200-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:34:11.268-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:34:11.274-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:42423"
time=2026-02-10T22:34:11.276-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:4 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:34:11.276-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:34:11.277-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4723576
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4612 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:34:14.311-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.13 seconds"
time=2026-02-10T22:34:14.311-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:34:14.311-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:34:14.312-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.14 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:34:15 | 200 |  5.129901459s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:34:40.914-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45587"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:34:41.990-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:34:41.990-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 36681"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:34:41.991-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:34:41.991-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:34:42.011-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:34:42.083-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:34:42.089-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:36681"
time=2026-02-10T22:34:42.100-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:34:42.101-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:34:42.104-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4696196
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4586 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:34:44.621-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.63 seconds"
time=2026-02-10T22:34:44.621-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:34:44.621-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:34:44.622-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.63 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:34:46 | 200 |  5.554388782s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:36:14.279-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38697"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:36:15.055-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:36:15.056-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 45145"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:36:15.057-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:36:15.057-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:36:15.077-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:36:15.145-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:36:15.151-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:45145"
time=2026-02-10T22:36:15.157-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:36:15.158-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:36:15.158-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4709040
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4598 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:36:17.421-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.36 seconds"
time=2026-02-10T22:36:17.421-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:36:17.421-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:36:17.422-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.37 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:36:19 | 200 |  5.023495616s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:39:29.052-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43577"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:39:29.624-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:39:29.624-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 39701"
time=2026-02-10T22:39:29.625-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:39:29.625-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:39:29.625-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:39:29.625-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:39:29.625-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:39:29.626-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:39:29.626-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:39:29.649-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:39:29.719-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:39:29.726-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:39701"
time=2026-02-10T22:39:29.737-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:39:29.737-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:39:29.738-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4681804
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4572 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:39:32.002-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.38 seconds"
time=2026-02-10T22:39:32.002-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:39:32.002-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:39:32.003-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.38 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:39:33 | 200 |  4.812285067s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:40:59.691-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38635"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:41:00.361-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:41:00.361-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 35523"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:41:00.362-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:41:00.362-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:41:00.385-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:41:00.454-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:41:00.461-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:35523"
time=2026-02-10T22:41:00.474-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:41:00.475-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:41:00.475-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4706268
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4595 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:41:02.740-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.38 seconds"
time=2026-02-10T22:41:02.740-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:41:02.740-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:41:02.741-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.38 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:41:04 | 200 |  5.270860449s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:42:41.767-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39197"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:42:42.273-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:42:42.273-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 37613"
time=2026-02-10T22:42:42.274-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:42:42.274-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:42:42.274-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:42:42.275-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:42:42.275-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:42:42.275-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:42:42.275-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:42:42.297-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:42:42.366-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:42:42.372-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:37613"
time=2026-02-10T22:42:42.375-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:42:42.375-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:42:42.376-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4700412
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4590 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-10T22:42:44.640-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.37 seconds"
time=2026-02-10T22:42:44.640-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-10T22:42:44.640-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:42:44.641-05:00 level=INFO source=server.go:1388 msg="llama runner started in 2.37 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/10 - 22:42:46 | 200 |  4.801684685s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:52:17.821-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35829"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:52:18.311-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:52:18.312-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 43113"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.5 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:52:18.313-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:52:18.313-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:52:18.333-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:52:18.400-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:52:18.406-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:43113"
time=2026-02-10T22:52:18.415-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:52:18.415-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:52:18.416-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4657012
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4547 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 867.61 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 909753600
ggml-backend.cpp:199: GGML_ASSERT(buffer) failed
[New LWP 193416]
[New LWP 193417]
[New LWP 193418]
[New LWP 193419]
[New LWP 193420]
[New LWP 193421]
[New LWP 193422]
[New LWP 193425]
time=2026-02-10T22:52:20.880-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
0x0000aaaad678116c in ?? ()
#0  0x0000aaaad678116c in ?? ()
#1  0x0000000000000080 in ?? ()
time=2026-02-10T22:52:21.293-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
[Inferior 1 (process 193415) detached]
SIGABRT: abort
PC=0xffff8cdf2008 m=5 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 13 gp=0x4000103180 m=5 mp=0x4000100008 [syscall]:
runtime.cgocall(0xaaaad73e5fbc, 0x400054ec08)
	runtime/cgocall.go:167 +0x44 fp=0x400054ebd0 sp=0x400054eb90 pc=0xaaaad6774914
github.com/ollama/ollama/llama._Cfunc_mtmd_init_from_file(0xffff30000bf0, 0xffff30000c70, {0x1, 0x1, 0x4, 0xaaaad7916eef, 0xaaaad7916ee3, 0xffffffff, 0x1, 0xffffffff, ...})
	_cgo_gotypes.go:1134 +0x34 fp=0x400054ec00 sp=0x400054ebd0 pc=0xaaaad6b2e4b4
github.com/ollama/ollama/llama.NewMtmdContext.func2(...)
	github.com/ollama/ollama/llama/llama.go:539
github.com/ollama/ollama/llama.NewMtmdContext(0x400034c010, {0x4000096540, 0x70})
	github.com/ollama/ollama/llama/llama.go:539 +0x12c fp=0x400054ed20 sp=0x400054ec00 pc=0xaaaad6b31c0c
github.com/ollama/ollama/runner/llamarunner.NewImageContext(0x400034c010, {0x4000096540, 0x70})
	github.com/ollama/ollama/runner/llamarunner/image.go:35 +0xe8 fp=0x400054eda0 sp=0x400054ed20 pc=0xaaaad6bd1328
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x400041be00, {{0x40005a3cb0, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x40005a3ca8, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:861 +0x1d8 fp=0x400054eef0 sp=0x400054eda0 pc=0xaaaad6bd5998
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0xb0 fp=0x400054efd0 sp=0x400054eef0 pc=0xaaaad6bd6730
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400054efd0 sp=0x400054efd0 pc=0xaaaad677fd14
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 10
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc

goroutine 1 gp=0x40000021c0 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000517710 sp=0x40005176f0 pc=0xaaaad6777e28
runtime.netpollblock(0x7000000000?, 0x6?, 0x0?)
	runtime/netpoll.go:575 +0x158 fp=0x4000517750 sp=0x4000517710 pc=0xaaaad673cde8
internal/poll.runtime_pollWait(0xffff45ac7f30, 0x72)
	runtime/netpoll.go:351 +0xa0 fp=0x4000517780 sp=0x4000517750 pc=0xaaaad6776fe0
internal/poll.(*pollDesc).wait(0x4000531900?, 0xaaaad671ed8c?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x40005177b0 sp=0x4000517780 pc=0xaaaad67f95f8
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0x4000531900)
	internal/poll/fd_unix.go:620 +0x24c fp=0x4000517860 sp=0x40005177b0 pc=0xaaaad67fdecc
net.(*netFD).accept(0x4000531900)
	net/fd_unix.go:172 +0x28 fp=0x4000517920 sp=0x4000517860 pc=0xaaaad686c9d8
net.(*TCPListener).accept(0x400051c880)
	net/tcpsock_posix.go:159 +0x24 fp=0x4000517970 sp=0x4000517920 pc=0xaaaad6881e74
net.(*TCPListener).Accept(0x400051c880)
	net/tcpsock.go:380 +0x2c fp=0x40005179b0 sp=0x4000517970 pc=0xaaaad6880e0c
net/http.(*onceCloseListener).Accept(0x400011e1b0?)
	<autogenerated>:1 +0x30 fp=0x40005179d0 sp=0x40005179b0 pc=0xaaaad6a5b360
net/http.(*Server).Serve(0x4000561300, {0xaaaad7c69da0, 0x400051c880})
	net/http/server.go:3424 +0x290 fp=0x4000517b00 sp=0x40005179d0 pc=0xaaaad6a34aa0
github.com/ollama/ollama/runner/llamarunner.Execute({0x4000032260, 0x4, 0x4})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1002 +0x7ac fp=0x4000517cd0 sp=0x4000517b00 pc=0xaaaad6bd6fcc
github.com/ollama/ollama/runner.Execute({0x4000032250?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0x16c fp=0x4000517d10 sp=0x4000517cd0 pc=0xaaaad6cc9a1c
github.com/ollama/ollama/cmd.NewCLI.func3(0x4000561100?, {0xaaaad76e10fd?, 0x4?, 0xaaaad76e1101?})
	github.com/ollama/ollama/cmd/cmd.go:1979 +0x54 fp=0x4000517d40 sp=0x4000517d10 pc=0xaaaad7386924
github.com/spf13/cobra.(*Command).execute(0x400049fb08, {0x400051c640, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x4000517e60 sp=0x4000517d40 pc=0xaaaad68dc6d8
github.com/spf13/cobra.(*Command).ExecuteC(0x4000528c08)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x4000517f20 sp=0x4000517e60 pc=0xaaaad68dce20
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x54 fp=0x4000517f40 sp=0x4000517f20 pc=0xaaaad7387474
runtime.main()
	runtime/proc.go:283 +0x284 fp=0x4000517fd0 sp=0x4000517f40 pc=0xaaaad6744194
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000517fd0 sp=0x4000517fd0 pc=0xaaaad677fd14

goroutine 2 gp=0x4000002c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ef90 sp=0x400005ef70 pc=0xaaaad6777e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0x400005efd0 sp=0x400005ef90 pc=0xaaaad67444e8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005efd0 sp=0x400005efd0 pc=0xaaaad677fd14
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x24

goroutine 3 gp=0x4000003180 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005f760 sp=0x400005f740 pc=0xaaaad6777e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0x400008a000)
	runtime/mgcsweep.go:316 +0x108 fp=0x400005f7b0 sp=0x400005f760 pc=0xaaaad672ed18
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x28 fp=0x400005f7d0 sp=0x400005f7b0 pc=0xaaaad6722b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005f7d0 sp=0x400005f7d0 pc=0xaaaad677fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x6c

goroutine 4 gp=0x4000003340 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xaaaad78ce828?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ff60 sp=0x400005ff40 pc=0xaaaad6777e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0xaaaad8572580)
	runtime/mgcscavenge.go:425 +0x5c fp=0x400005ff90 sp=0x400005ff60 pc=0xaaaad672c7dc
runtime.bgscavenge(0x400008a000)
	runtime/mgcscavenge.go:658 +0xac fp=0x400005ffb0 sp=0x400005ff90 pc=0xaaaad672cd5c
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x28 fp=0x400005ffd0 sp=0x400005ffb0 pc=0xaaaad6722ae8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005ffd0 sp=0x400005ffd0 pc=0xaaaad677fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xac

goroutine 5 gp=0x4000003c00 m=nil [finalizer wait]:
runtime.gopark(0x18000001b8?, 0x1000000000000?, 0xf8?, 0xe5?, 0xaaaad6a5ddac?)
	runtime/proc.go:435 +0xc8 fp=0x400005e590 sp=0x400005e570 pc=0xaaaad6777e28
runtime.runfinq()
	runtime/mfinal.go:196 +0x108 fp=0x400005e7d0 sp=0x400005e590 pc=0xaaaad6721b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005e7d0 sp=0x400005e7d0 pc=0xaaaad677fd14
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x80

goroutine 6 gp=0x40001dc700 m=nil [chan receive]:
runtime.gopark(0x40002234a0?, 0x400050e018?, 0x48?, 0x7?, 0xaaaad6844f98?)
	runtime/proc.go:435 +0xc8 fp=0x40000606f0 sp=0x40000606d0 pc=0xaaaad6777e28
runtime.chanrecv(0x4000096310, 0x0, 0x1)
	runtime/chan.go:664 +0x42c fp=0x4000060770 sp=0x40000606f0 pc=0xaaaad6713bac
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x14 fp=0x40000607a0 sp=0x4000060770 pc=0xaaaad6713744
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x3c fp=0x40000607d0 sp=0x40000607a0 pc=0xaaaad6725d6c
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000607d0 sp=0x40000607d0 pc=0xaaaad677fd14
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x78

goroutine 7 gp=0x40001dcfc0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000060f10 sp=0x4000060ef0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x4000060fb0 sp=0x4000060f10 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000060fd0 sp=0x4000060fb0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000060fd0 sp=0x4000060fd0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 18 gp=0x4000102380 m=nil [GC worker (idle)]:
runtime.gopark(0x9ed679032192?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005a710 sp=0x400005a6f0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x400005a7b0 sp=0x400005a710 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005a7d0 sp=0x400005a7b0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005a7d0 sp=0x400005a7d0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 34 gp=0x4000504000 m=nil [GC worker (idle)]:
runtime.gopark(0x9ed679043dd4?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000550f10 sp=0x4000550ef0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x4000550fb0 sp=0x4000550f10 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000550fd0 sp=0x4000550fb0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000550fd0 sp=0x4000550fd0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 8 gp=0x40001dd180 m=nil [GC worker (idle)]:
runtime.gopark(0x9ed679044c34?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400054cf10 sp=0x400054cef0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x400054cfb0 sp=0x400054cf10 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400054cfd0 sp=0x400054cfb0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400054cfd0 sp=0x400054cfd0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 19 gp=0x4000102540 m=nil [GC worker (idle)]:
runtime.gopark(0x9ed67903c474?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400006ef10 sp=0x400006eef0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x400006efb0 sp=0x400006ef10 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400006efd0 sp=0x400006efb0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400006efd0 sp=0x400006efd0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 35 gp=0x40005041c0 m=nil [GC worker (idle)]:
runtime.gopark(0x9ed67902c1f2?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400050af10 sp=0x400050aef0 pc=0xaaaad6777e28
runtime.gcBgMarkWorker(0x40000978f0)
	runtime/mgc.go:1423 +0xdc fp=0x400050afb0 sp=0x400050af10 pc=0xaaaad6724fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400050afd0 sp=0x400050afb0 pc=0xaaaad6724ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400050afd0 sp=0x400050afd0 pc=0xaaaad677fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 9 gp=0x4000102c40 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0xaaaad8584280?, 0x0?, 0x80?, 0xa1?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005be20 sp=0x400005be00 pc=0xaaaad6777e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0x400041be20, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x204 fp=0x400005be70 sp=0x400005be20 pc=0xaaaad6758634
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x2c fp=0x400005beb0 sp=0x400005be70 pc=0xaaaad67797dc
sync.(*WaitGroup).Wait(0x400041be18)
	sync/waitgroup.go:118 +0x70 fp=0x400005bed0 sp=0x400005beb0 pc=0xaaaad678b3f0
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0x400041be00, {0xaaaad7c6c480, 0x4000379a40})
	github.com/ollama/ollama/runner/llamarunner/runner.go:360 +0x40 fp=0x400005bfa0 sp=0x400005bed0 pc=0xaaaad6bd2bc0
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x30 fp=0x400005bfd0 sp=0x400005bfa0 pc=0xaaaad6bd71f0
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005bfd0 sp=0x400005bfd0 pc=0xaaaad677fd14
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x44c

goroutine 23 gp=0x4000504a80 m=nil [IO wait]:
runtime.gopark(0x40004f9908?, 0xaaaad67fc558?, 0x40?, 0x0?, 0x2a?)
	runtime/proc.go:435 +0xc8 fp=0x40004f98e0 sp=0x40004f98c0 pc=0xaaaad6777e28
runtime.netpollblock(0x0?, 0xffffffff?, 0xff?)
	runtime/netpoll.go:575 +0x158 fp=0x40004f9920 sp=0x40004f98e0 pc=0xaaaad673cde8
internal/poll.runtime_pollWait(0xffff45ac7d00, 0x72)
	runtime/netpoll.go:351 +0xa0 fp=0x40004f9950 sp=0x40004f9920 pc=0xaaaad6776fe0
internal/poll.(*pollDesc).wait(0x40005a4180?, 0x4000128000?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x40004f9980 sp=0x40004f9950 pc=0xaaaad67f95f8
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0x40005a4180, {0x4000128000, 0x1000, 0x1000})
	internal/poll/fd_unix.go:165 +0x1fc fp=0x40004f9a20 sp=0x40004f9980 pc=0xaaaad67fa8ac
net.(*netFD).Read(0x40005a4180, {0x4000128000?, 0x40004f9ab8?, 0xaaaad6a2a9fc?})
	net/fd_posix.go:55 +0x28 fp=0x40004f9a70 sp=0x40004f9a20 pc=0xaaaad686afa8
net.(*conn).Read(0x4000532080, {0x4000128000?, 0x72?, 0x0?})
	net/net.go:194 +0x34 fp=0x40004f9ac0 sp=0x40004f9a70 pc=0xaaaad68786e4
net/http.(*connReader).Read(0x4000114300, {0x4000128000, 0x1000, 0x1000})
	net/http/server.go:798 +0x234 fp=0x40004f9b20 sp=0x40004f9ac0 pc=0xaaaad6a2ac04
bufio.(*Reader).fill(0x4000590060)
	bufio/bufio.go:113 +0xf8 fp=0x40004f9b60 sp=0x40004f9b20 pc=0xaaaad688f058
bufio.(*Reader).Peek(0x4000590060, 0x4)
	bufio/bufio.go:152 +0x60 fp=0x40004f9b80 sp=0x40004f9b60 pc=0xaaaad688f1c0
net/http.(*conn).serve(0x400011e1b0, {0xaaaad7c6c448, 0x400021edb0})
	net/http/server.go:2137 +0x664 fp=0x40004f9fa0 sp=0x40004f9b80 pc=0xaaaad6a2fda4
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3454 +0x30 fp=0x40004f9fd0 sp=0x40004f9fa0 pc=0xaaaad6a34e30
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40004f9fd0 sp=0x40004f9fd0 pc=0xaaaad677fd14
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3454 +0x3d8

r0      0x0
r1      0x2f38b
r2      0x6
r3      0xffff44a5f0c0
r4      0xffff8d31ab58
r5      0xffff44a5d7f0
r6      0xfffffff8
r7      0xffff44a5d7d0
r8      0x83
r9      0x0
r10     0xffff8cd74860
r11     0x0
r12     0x63746977735f7478
r13     0x0
r14     0x3120363131203430
r15     0x3120353331203032
r16     0x1
r17     0xaaaad84b0988
r18     0x1
r19     0x2f38b
r20     0xffff44a5f0c0
r21     0x6
r22     0x1c
r23     0xaaaad7912954
r24     0xaaaad790d981
r25     0xffff44a5e248
r26     0x1a40
r27     0xffff33264900
r28     0xaaaad7912968
r29     0xffff44a5d6e0
lr      0xffff8cdf1ff4
sp      0xffff44a5d650
pc      0xffff8cdf2008
fault   0x0
time=2026-02-10T22:52:21.496-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-10T22:52:21.544-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: GGML_ASSERT(buffer) failed"
[GIN] 2026/02/10 - 22:52:21 | 500 |  3.886561533s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:58:31.896-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40283"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:58:32.313-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:58:32.313-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 35423"
time=2026-02-10T22:58:32.314-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="3.7 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:58:32.314-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.3 GiB" free="3.7 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:58:32.314-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:58:32.315-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:58:32.315-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:58:32.315-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:58:32.315-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:58:32.336-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:58:32.408-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:58:32.415-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:35423"
time=2026-02-10T22:58:32.425-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:58:32.426-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:58:32.426-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3939152
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 3846 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 867.61 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 909753600
ggml-backend.cpp:199: GGML_ASSERT(buffer) failed
[New LWP 199303]
[New LWP 199304]
[New LWP 199305]
[New LWP 199306]
[New LWP 199307]
[New LWP 199309]
[New LWP 199310]
[New LWP 199311]
[New LWP 199318]
time=2026-02-10T22:58:34.390-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
0x0000aaaac018116c in ?? ()
#0  0x0000aaaac018116c in ?? ()
#1  0x0000000000000080 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
[Inferior 1 (process 199302) detached]
SIGABRT: abort
PC=0xffffb4252008 m=8 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 52 gp=0x40000b0540 m=8 mp=0x40000ae008 [syscall]:
runtime.cgocall(0xaaaac0de5fbc, 0x40005d9c08)
	runtime/cgocall.go:167 +0x44 fp=0x40005d9bd0 sp=0x40005d9b90 pc=0xaaaac0174914
github.com/ollama/ollama/llama._Cfunc_mtmd_init_from_file(0xffff48000bf0, 0xffff48000c70, {0x1, 0x1, 0x4, 0xaaaac1316eef, 0xaaaac1316ee3, 0xffffffff, 0x1, 0xffffffff, ...})
	_cgo_gotypes.go:1134 +0x34 fp=0x40005d9c00 sp=0x40005d9bd0 pc=0xaaaac052e4b4
github.com/ollama/ollama/llama.NewMtmdContext.func2(...)
	github.com/ollama/ollama/llama/llama.go:539
github.com/ollama/ollama/llama.NewMtmdContext(0x4000348050, {0x4000180150, 0x70})
	github.com/ollama/ollama/llama/llama.go:539 +0x12c fp=0x40005d9d20 sp=0x40005d9c00 pc=0xaaaac0531c0c
github.com/ollama/ollama/runner/llamarunner.NewImageContext(0x4000348050, {0x4000180150, 0x70})
	github.com/ollama/ollama/runner/llamarunner/image.go:35 +0xe8 fp=0x40005d9da0 sp=0x40005d9d20 pc=0xaaaac05d1328
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x40000ddd60, {{0x40001e0fe0, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x40001e0fb8, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:861 +0x1d8 fp=0x40005d9ef0 sp=0x40005d9da0 pc=0xaaaac05d5998
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0xb0 fp=0x40005d9fd0 sp=0x40005d9ef0 pc=0xaaaac05d6730
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40005d9fd0 sp=0x40005d9fd0 pc=0xaaaac017fd14
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 10
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc

goroutine 1 gp=0x40000021c0 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40003c3710 sp=0x40003c36f0 pc=0xaaaac0177e28
runtime.netpollblock(0x7000000000?, 0x6?, 0x0?)
	runtime/netpoll.go:575 +0x158 fp=0x40003c3750 sp=0x40003c3710 pc=0xaaaac013cde8
internal/poll.runtime_pollWait(0xffff6c6d8de0, 0x72)
	runtime/netpoll.go:351 +0xa0 fp=0x40003c3780 sp=0x40003c3750 pc=0xaaaac0176fe0
internal/poll.(*pollDesc).wait(0x40005b3400?, 0xaaaac0200058?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x40003c37b0 sp=0x40003c3780 pc=0xaaaac01f95f8
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0x40005b3400)
	internal/poll/fd_unix.go:620 +0x24c fp=0x40003c3860 sp=0x40003c37b0 pc=0xaaaac01fdecc
net.(*netFD).accept(0x40005b3400)
	net/fd_unix.go:172 +0x28 fp=0x40003c3920 sp=0x40003c3860 pc=0xaaaac026c9d8
net.(*TCPListener).accept(0x400008ce40)
	net/tcpsock_posix.go:159 +0x24 fp=0x40003c3970 sp=0x40003c3920 pc=0xaaaac0281e74
net.(*TCPListener).Accept(0x400008ce40)
	net/tcpsock.go:380 +0x2c fp=0x40003c39b0 sp=0x40003c3970 pc=0xaaaac0280e0c
net/http.(*onceCloseListener).Accept(0x40000e2480?)
	<autogenerated>:1 +0x30 fp=0x40003c39d0 sp=0x40003c39b0 pc=0xaaaac045b360
net/http.(*Server).Serve(0x40001ff900, {0xaaaac1669da0, 0x400008ce40})
	net/http/server.go:3424 +0x290 fp=0x40003c3b00 sp=0x40003c39d0 pc=0xaaaac0434aa0
github.com/ollama/ollama/runner/llamarunner.Execute({0x4000130140, 0x4, 0x4})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1002 +0x7ac fp=0x40003c3cd0 sp=0x40003c3b00 pc=0xaaaac05d6fcc
github.com/ollama/ollama/runner.Execute({0x4000130130?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0x16c fp=0x40003c3d10 sp=0x40003c3cd0 pc=0xaaaac06c9a1c
github.com/ollama/ollama/cmd.NewCLI.func3(0x40001ff700?, {0xaaaac10e10fd?, 0x4?, 0xaaaac10e1101?})
	github.com/ollama/ollama/cmd/cmd.go:1979 +0x54 fp=0x40003c3d40 sp=0x40003c3d10 pc=0xaaaac0d86924
github.com/spf13/cobra.(*Command).execute(0x40000e5508, {0x400008cc00, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x40003c3e60 sp=0x40003c3d40 pc=0xaaaac02dc6d8
github.com/spf13/cobra.(*Command).ExecuteC(0x40005c6908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x40003c3f20 sp=0x40003c3e60 pc=0xaaaac02dce20
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x54 fp=0x40003c3f40 sp=0x40003c3f20 pc=0xaaaac0d87474
runtime.main()
	runtime/proc.go:283 +0x284 fp=0x40003c3fd0 sp=0x40003c3f40 pc=0xaaaac0144194
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40003c3fd0 sp=0x40003c3fd0 pc=0xaaaac017fd14

goroutine 2 gp=0x4000002c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ef90 sp=0x400005ef70 pc=0xaaaac0177e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0x400005efd0 sp=0x400005ef90 pc=0xaaaac01444e8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005efd0 sp=0x400005efd0 pc=0xaaaac017fd14
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x24

goroutine 3 gp=0x4000003180 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005f760 sp=0x400005f740 pc=0xaaaac0177e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0x400008a000)
	runtime/mgcsweep.go:316 +0x108 fp=0x400005f7b0 sp=0x400005f760 pc=0xaaaac012ed18
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x28 fp=0x400005f7d0 sp=0x400005f7b0 pc=0xaaaac0122b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005f7d0 sp=0x400005f7d0 pc=0xaaaac017fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x6c

goroutine 4 gp=0x4000003340 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xaaaac12ce828?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ff60 sp=0x400005ff40 pc=0xaaaac0177e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0xaaaac1f72580)
	runtime/mgcscavenge.go:425 +0x5c fp=0x400005ff90 sp=0x400005ff60 pc=0xaaaac012c7dc
runtime.bgscavenge(0x400008a000)
	runtime/mgcscavenge.go:658 +0xac fp=0x400005ffb0 sp=0x400005ff90 pc=0xaaaac012cd5c
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x28 fp=0x400005ffd0 sp=0x400005ffb0 pc=0xaaaac0122ae8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005ffd0 sp=0x400005ffd0 pc=0xaaaac017fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xac

goroutine 18 gp=0x4000102700 m=nil [finalizer wait]:
runtime.gopark(0x18000001b8?, 0x1000000000000?, 0xf8?, 0xe5?, 0xaaaac045ddac?)
	runtime/proc.go:435 +0xc8 fp=0x400005e590 sp=0x400005e570 pc=0xaaaac0177e28
runtime.runfinq()
	runtime/mfinal.go:196 +0x108 fp=0x400005e7d0 sp=0x400005e590 pc=0xaaaac0121b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005e7d0 sp=0x400005e7d0 pc=0xaaaac017fd14
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x80

goroutine 19 gp=0x4000103180 m=nil [chan receive]:
runtime.gopark(0x400023b680?, 0x4000510300?, 0x48?, 0xa7?, 0xaaaac0244f98?)
	runtime/proc.go:435 +0xc8 fp=0x400005a6f0 sp=0x400005a6d0 pc=0xaaaac0177e28
runtime.chanrecv(0x4000110310, 0x0, 0x1)
	runtime/chan.go:664 +0x42c fp=0x400005a770 sp=0x400005a6f0 pc=0xaaaac0113bac
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x14 fp=0x400005a7a0 sp=0x400005a770 pc=0xaaaac0113744
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x3c fp=0x400005a7d0 sp=0x400005a7a0 pc=0xaaaac0125d6c
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005a7d0 sp=0x400005a7d0 pc=0xaaaac017fd14
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x78

goroutine 20 gp=0x40001036c0 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005af10 sp=0x400005aef0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x400005afb0 sp=0x400005af10 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005afd0 sp=0x400005afb0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005afd0 sp=0x400005afd0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 34 gp=0x4000504000 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400050a710 sp=0x400050a6f0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x400050a7b0 sp=0x400050a710 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400050a7d0 sp=0x400050a7b0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400050a7d0 sp=0x400050a7d0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 5 gp=0x4000003880 m=nil [GC worker (idle)]:
runtime.gopark(0x9f2db0f54dba?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40005d6f10 sp=0x40005d6ef0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x40005d6fb0 sp=0x40005d6f10 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40005d6fd0 sp=0x40005d6fb0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40005d6fd0 sp=0x40005d6fd0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 6 gp=0x4000003a40 m=nil [GC worker (idle)]:
runtime.gopark(0x9f2db0dd2213?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000060f10 sp=0x4000060ef0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x4000060fb0 sp=0x4000060f10 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000060fd0 sp=0x4000060fb0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000060fd0 sp=0x4000060fd0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 7 gp=0x4000003c00 m=nil [GC worker (idle)]:
runtime.gopark(0x9f2d8d4d091a?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000061710 sp=0x40000616f0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x40000617b0 sp=0x4000061710 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40000617d0 sp=0x40000617b0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000617d0 sp=0x40000617d0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 8 gp=0x4000003dc0 m=nil [GC worker (idle)]:
runtime.gopark(0x9f2db0f548fa?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000061f10 sp=0x4000061ef0 pc=0xaaaac0177e28
runtime.gcBgMarkWorker(0x40001118f0)
	runtime/mgc.go:1423 +0xdc fp=0x4000061fb0 sp=0x4000061f10 pc=0xaaaac0124fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000061fd0 sp=0x4000061fb0 pc=0xaaaac0124ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000061fd0 sp=0x4000061fd0 pc=0xaaaac017fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 9 gp=0x4000103dc0 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0xaaaac1f83000?, 0x0?, 0x80?, 0x21?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400050de20 sp=0x400050de00 pc=0xaaaac0177e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0x40000ddd80, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x204 fp=0x400050de70 sp=0x400050de20 pc=0xaaaac0158634
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x2c fp=0x400050deb0 sp=0x400050de70 pc=0xaaaac01797dc
sync.(*WaitGroup).Wait(0x40000ddd78)
	sync/waitgroup.go:118 +0x70 fp=0x400050ded0 sp=0x400050deb0 pc=0xaaaac018b3f0
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0x40000ddd60, {0xaaaac166c480, 0x40005a3040})
	github.com/ollama/ollama/runner/llamarunner/runner.go:360 +0x40 fp=0x400050dfa0 sp=0x400050ded0 pc=0xaaaac05d2bc0
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x30 fp=0x400050dfd0 sp=0x400050dfa0 pc=0xaaaac05d71f0
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400050dfd0 sp=0x400050dfd0 pc=0xaaaac017fd14
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x44c

r0      0x0
r1      0x30a8e
r2      0x6
r3      0xffff65fbf0c0
r4      0xffffb4770b58
r5      0xffff65fbd7f0
r6      0xfffffff8
r7      0xffff65fbd7d0
r8      0x83
r9      0x0
r10     0xffffb41d4860
r11     0x0
r12     0x63746977735f7478
r13     0x0
r14     0x3120363131203430
r15     0x3120353331203032
r16     0x1
r17     0xaaaac1eb0988
r18     0x1
r19     0x30a8e
r20     0xffff65fbf0c0
r21     0x6
r22     0x1c
r23     0xaaaac1312954
r24     0xaaaac130d981
r25     0xffff65fbe248
r26     0x1a40
r27     0xffff4af813a0
r28     0xaaaac1312968
r29     0xffff65fbd6e0
lr      0xffffb4251ff4
sp      0xffff65fbd650
pc      0xffffb4252008
fault   0x0
time=2026-02-10T22:58:34.676-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-10T22:58:34.893-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: GGML_ASSERT(buffer) failed"
[GIN] 2026/02/10 - 22:58:34 | 500 |  3.205147487s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-10T22:58:42.485-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33569"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-10T22:58:42.935-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-10T22:58:42.936-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 35589"
time=2026-02-10T22:58:42.936-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="3.7 GiB" free_swap="3.6 GiB"
time=2026-02-10T22:58:42.936-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.3 GiB" free="3.7 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-10T22:58:42.937-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-10T22:58:42.937-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-10T22:58:42.937-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-10T22:58:42.937-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-10T22:58:42.937-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-10T22:58:42.958-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-10T22:58:43.027-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-10T22:58:43.032-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:35589"
time=2026-02-10T22:58:43.036-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-10T22:58:43.037-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-10T22:58:43.037-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3931556
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 3839 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 867.61 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 909753600
ggml-backend.cpp:199: GGML_ASSERT(buffer) failed
[New LWP 199580]
[New LWP 199581]
[New LWP 199582]
[New LWP 199583]
[New LWP 199584]
[New LWP 199585]
[New LWP 199586]
[New LWP 199587]
[New LWP 199590]
time=2026-02-10T22:58:44.998-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
0x0000aaaaea26116c in ?? ()
#0  0x0000aaaaea26116c in ?? ()
#1  0x0000000000000080 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
[Inferior 1 (process 199579) detached]
SIGABRT: abort
PC=0xffffb8a12008 m=9 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 9 gp=0x4000583880 m=9 mp=0x4000580808 [syscall]:
runtime.cgocall(0xaaaaeaec5fbc, 0x40005cdc08)
	runtime/cgocall.go:167 +0x44 fp=0x40005cdbd0 sp=0x40005cdb90 pc=0xaaaaea254914
github.com/ollama/ollama/llama._Cfunc_mtmd_init_from_file(0xffff48000bf0, 0xffff48000c70, {0x1, 0x1, 0x4, 0xaaaaeb3f6eef, 0xaaaaeb3f6ee3, 0xffffffff, 0x1, 0xffffffff, ...})
	_cgo_gotypes.go:1134 +0x34 fp=0x40005cdc00 sp=0x40005cdbd0 pc=0xaaaaea60e4b4
github.com/ollama/ollama/llama.NewMtmdContext.func2(...)
	github.com/ollama/ollama/llama/llama.go:539
github.com/ollama/ollama/llama.NewMtmdContext(0x40001e0e10, {0x4000118700, 0x70})
	github.com/ollama/ollama/llama/llama.go:539 +0x12c fp=0x40005cdd20 sp=0x40005cdc00 pc=0xaaaaea611c0c
github.com/ollama/ollama/runner/llamarunner.NewImageContext(0x40001e0e10, {0x4000118700, 0x70})
	github.com/ollama/ollama/runner/llamarunner/image.go:35 +0xe8 fp=0x40005cdda0 sp=0x40005cdd20 pc=0xaaaaea6b1328
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x40000bfcc0, {{0x40001e0840, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x40001e0838, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:861 +0x1d8 fp=0x40005cdef0 sp=0x40005cdda0 pc=0xaaaaea6b5998
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0xb0 fp=0x40005cdfd0 sp=0x40005cdef0 pc=0xaaaaea6b6730
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40005cdfd0 sp=0x40005cdfd0 pc=0xaaaaea25fd14
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 6
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc

goroutine 1 gp=0x40000021c0 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000099710 sp=0x40000996f0 pc=0xaaaaea257e28
runtime.netpollblock(0x7000000000?, 0x6?, 0x0?)
	runtime/netpoll.go:575 +0x158 fp=0x4000099750 sp=0x4000099710 pc=0xaaaaea21cde8
internal/poll.runtime_pollWait(0xffff70688de0, 0x72)
	runtime/netpoll.go:351 +0xa0 fp=0x4000099780 sp=0x4000099750 pc=0xaaaaea256fe0
internal/poll.(*pollDesc).wait(0x40001dfc80?, 0xaaaaea2e0058?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x40000997b0 sp=0x4000099780 pc=0xaaaaea2d95f8
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0x40001dfc80)
	internal/poll/fd_unix.go:620 +0x24c fp=0x4000099860 sp=0x40000997b0 pc=0xaaaaea2ddecc
net.(*netFD).accept(0x40001dfc80)
	net/fd_unix.go:172 +0x28 fp=0x4000099920 sp=0x4000099860 pc=0xaaaaea34c9d8
net.(*TCPListener).accept(0x40004a5300)
	net/tcpsock_posix.go:159 +0x24 fp=0x4000099970 sp=0x4000099920 pc=0xaaaaea361e74
net.(*TCPListener).Accept(0x40004a5300)
	net/tcpsock.go:380 +0x2c fp=0x40000999b0 sp=0x4000099970 pc=0xaaaaea360e0c
net/http.(*onceCloseListener).Accept(0x40000c4480?)
	<autogenerated>:1 +0x30 fp=0x40000999d0 sp=0x40000999b0 pc=0xaaaaea53b360
net/http.(*Server).Serve(0x40001ff900, {0xaaaaeb749da0, 0x40004a5300})
	net/http/server.go:3424 +0x290 fp=0x4000099b00 sp=0x40000999d0 pc=0xaaaaea514aa0
github.com/ollama/ollama/runner/llamarunner.Execute({0x4000116200, 0x4, 0x4})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1002 +0x7ac fp=0x4000099cd0 sp=0x4000099b00 pc=0xaaaaea6b6fcc
github.com/ollama/ollama/runner.Execute({0x40001161f0?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0x16c fp=0x4000099d10 sp=0x4000099cd0 pc=0xaaaaea7a9a1c
github.com/ollama/ollama/cmd.NewCLI.func3(0x40001ff700?, {0xaaaaeb1c10fd?, 0x4?, 0xaaaaeb1c1101?})
	github.com/ollama/ollama/cmd/cmd.go:1979 +0x54 fp=0x4000099d40 sp=0x4000099d10 pc=0xaaaaeae66924
github.com/spf13/cobra.(*Command).execute(0x40000c7508, {0x40004a50c0, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x4000099e60 sp=0x4000099d40 pc=0xaaaaea3bc6d8
github.com/spf13/cobra.(*Command).ExecuteC(0x40000a0908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x4000099f20 sp=0x4000099e60 pc=0xaaaaea3bce20
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x54 fp=0x4000099f40 sp=0x4000099f20 pc=0xaaaaeae67474
runtime.main()
	runtime/proc.go:283 +0x284 fp=0x4000099fd0 sp=0x4000099f40 pc=0xaaaaea224194
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000099fd0 sp=0x4000099fd0 pc=0xaaaaea25fd14

goroutine 2 gp=0x4000002c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ef90 sp=0x400005ef70 pc=0xaaaaea257e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0x400005efd0 sp=0x400005ef90 pc=0xaaaaea2244e8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005efd0 sp=0x400005efd0 pc=0xaaaaea25fd14
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x24

goroutine 18 gp=0x4000102380 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005a760 sp=0x400005a740 pc=0xaaaaea257e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0x4000110000)
	runtime/mgcsweep.go:316 +0x108 fp=0x400005a7b0 sp=0x400005a760 pc=0xaaaaea20ed18
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x28 fp=0x400005a7d0 sp=0x400005a7b0 pc=0xaaaaea202b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005a7d0 sp=0x400005a7d0 pc=0xaaaaea25fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x6c

goroutine 19 gp=0x4000102540 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xaaaaeb3ae828?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005af60 sp=0x400005af40 pc=0xaaaaea257e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0xaaaaec052580)
	runtime/mgcscavenge.go:425 +0x5c fp=0x400005af90 sp=0x400005af60 pc=0xaaaaea20c7dc
runtime.bgscavenge(0x4000110000)
	runtime/mgcscavenge.go:658 +0xac fp=0x400005afb0 sp=0x400005af90 pc=0xaaaaea20cd5c
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x28 fp=0x400005afd0 sp=0x400005afb0 pc=0xaaaaea202ae8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005afd0 sp=0x400005afd0 pc=0xaaaaea25fd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xac

goroutine 20 gp=0x4000102a80 m=nil [finalizer wait]:
runtime.gopark(0x18000001b8?, 0x1000000000000?, 0xf8?, 0xe5?, 0xaaaaea53ddac?)
	runtime/proc.go:435 +0xc8 fp=0x400005e590 sp=0x400005e570 pc=0xaaaaea257e28
runtime.runfinq()
	runtime/mfinal.go:196 +0x108 fp=0x400005e7d0 sp=0x400005e590 pc=0xaaaaea201b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005e7d0 sp=0x400005e7d0 pc=0xaaaaea25fd14
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x80

goroutine 21 gp=0x4000103500 m=nil [chan receive]:
runtime.gopark(0x40002377c0?, 0x4000590000?, 0x48?, 0xb7?, 0xaaaaea324f98?)
	runtime/proc.go:435 +0xc8 fp=0x400005b6f0 sp=0x400005b6d0 pc=0xaaaaea257e28
runtime.chanrecv(0x4000118310, 0x0, 0x1)
	runtime/chan.go:664 +0x42c fp=0x400005b770 sp=0x400005b6f0 pc=0xaaaaea1f3bac
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x14 fp=0x400005b7a0 sp=0x400005b770 pc=0xaaaaea1f3744
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x3c fp=0x400005b7d0 sp=0x400005b7a0 pc=0xaaaaea205d6c
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005b7d0 sp=0x400005b7d0 pc=0xaaaaea25fd14
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x78

goroutine 22 gp=0x4000103880 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005bf10 sp=0x400005bef0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005bfb0 sp=0x400005bf10 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005bfd0 sp=0x400005bfb0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005bfd0 sp=0x400005bfd0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 23 gp=0x4000103a40 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005c710 sp=0x400005c6f0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005c7b0 sp=0x400005c710 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005c7d0 sp=0x400005c7b0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005c7d0 sp=0x400005c7d0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 3 gp=0x4000003500 m=nil [GC worker (idle)]:
runtime.gopark(0x9f3006771346?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005f710 sp=0x400005f6f0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005f7b0 sp=0x400005f710 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005f7d0 sp=0x400005f7b0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005f7d0 sp=0x400005f7d0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 4 gp=0x40000036c0 m=nil [GC worker (idle)]:
runtime.gopark(0x9f30067723e6?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ff10 sp=0x400005fef0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005ffb0 sp=0x400005ff10 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005ffd0 sp=0x400005ffb0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005ffd0 sp=0x400005ffd0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 34 gp=0x400029e000 m=nil [GC worker (idle)]:
runtime.gopark(0x9f3006769b25?, 0x3?, 0x68?, 0x11?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40002a4710 sp=0x40002a46f0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x40002a47b0 sp=0x40002a4710 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40002a47d0 sp=0x40002a47b0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40002a47d0 sp=0x40002a47d0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 24 gp=0x4000103c00 m=nil [GC worker (idle)]:
runtime.gopark(0x9f30067772a7?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005cf10 sp=0x400005cef0 pc=0xaaaaea257e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005cfb0 sp=0x400005cf10 pc=0xaaaaea204fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005cfd0 sp=0x400005cfb0 pc=0xaaaaea204ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005cfd0 sp=0x400005cfd0 pc=0xaaaaea25fd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 5 gp=0x4000583340 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0xaaaaec060d80?, 0x0?, 0x0?, 0x60?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40002a7620 sp=0x40002a7600 pc=0xaaaaea257e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0x40000bfce0, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x204 fp=0x40002a7670 sp=0x40002a7620 pc=0xaaaaea238634
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x2c fp=0x40002a76b0 sp=0x40002a7670 pc=0xaaaaea2597dc
sync.(*WaitGroup).Wait(0x40000bfcd8)
	sync/waitgroup.go:118 +0x70 fp=0x40002a76d0 sp=0x40002a76b0 pc=0xaaaaea26b3f0
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0x40000bfcc0, {0xaaaaeb74c480, 0x4000402c80})
	github.com/ollama/ollama/runner/llamarunner/runner.go:360 +0x40 fp=0x40002a77a0 sp=0x40002a76d0 pc=0xaaaaea6b2bc0
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x30 fp=0x40002a77d0 sp=0x40002a77a0 pc=0xaaaaea6b71f0
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40002a77d0 sp=0x40002a77d0 pc=0xaaaaea25fd14
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x44c

r0      0x0
r1      0x30ba3
r2      0x6
r3      0xffff69fbf0c0
r4      0xffffb8f35b58
r5      0xffff69fbd7f0
r6      0xfffffff8
r7      0xffff69fbd7d0
r8      0x83
r9      0x0
r10     0xffffb8994860
r11     0x0
r12     0x63746977735f7478
r13     0x0
r14     0x3120363131203430
r15     0x3120353331203032
r16     0x1
r17     0xaaaaebf90988
r18     0x1
r19     0x30ba3
r20     0xffff69fbf0c0
r21     0x6
r22     0x1c
r23     0xaaaaeb3f2954
r24     0xaaaaeb3ed981
r25     0xffff69fbe248
r26     0x1a40
r27     0xffff4af816e0
r28     0xaaaaeb3f2968
r29     0xffff69fbd6e0
lr      0xffffb8a11ff4
sp      0xffff69fbd650
pc      0xffffb8a12008
fault   0x0
time=2026-02-10T22:58:45.327-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-10T22:58:45.500-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: GGML_ASSERT(buffer) failed"
[GIN] 2026/02/10 - 22:58:45 | 500 |  3.210465259s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T07:56:26.793-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32775"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T07:56:27.197-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T07:56:27.197-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 44591"
time=2026-02-11T07:56:27.198-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.1 GiB" free_swap="3.6 GiB"
time=2026-02-11T07:56:27.198-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.6 GiB" free="4.1 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T07:56:27.198-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T07:56:27.199-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T07:56:27.199-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T07:56:27.199-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T07:56:27.199-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T07:56:27.219-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T07:56:27.289-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T07:56:27.295-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:44591"
time=2026-02-11T07:56:27.299-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T07:56:27.300-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:56:27.301-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4345888
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4244 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 384.00 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 402653184
llama_init_from_model: failed to initialize the context: failed to allocate buffer for kv cache
panic: unable to create llama context

goroutine 23 [running]:
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x4000604320, {{0x40001d8fe0, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x40001d8fb8, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:849 +0x298
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 51
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc
time=2026-02-11T07:56:29.571-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
time=2026-02-11T07:56:29.619-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-11T07:56:29.822-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: cudaMalloc failed: out of memory"
[GIN] 2026/02/11 - 07:56:29 | 500 |  3.178556889s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T07:56:37.280-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38607"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T07:56:37.752-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T07:56:37.753-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 32963"
time=2026-02-11T07:56:37.753-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="3.5 GiB" free_swap="3.6 GiB"
time=2026-02-11T07:56:37.753-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.0 GiB" free="3.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T07:56:37.753-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T07:56:37.754-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T07:56:37.754-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T07:56:37.754-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T07:56:37.754-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T07:56:37.775-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T07:56:37.843-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T07:56:37.849-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:32963"
time=2026-02-11T07:56:37.854-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T07:56:37.855-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:56:37.855-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3691884
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 3605 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 384.00 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 402653184
llama_init_from_model: failed to initialize the context: failed to allocate buffer for kv cache
panic: unable to create llama context

goroutine 20 [running]:
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x40002f1180, {{0x400059e7d0, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x400059e7c8, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:849 +0x298
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 12
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc
time=2026-02-11T07:56:38.795-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-11T07:56:38.861-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: cudaMalloc failed: out of memory"
[GIN] 2026/02/11 - 07:56:38 | 500 |   1.73572659s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-11T07:58:39.534-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-11T07:58:39.722-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-11T07:58:39.723-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-11T07:58:39.725-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-11T07:58:39.790-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-11T07:58:39.869-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39171"
time=2026-02-11T07:58:42.806-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46507"
time=2026-02-11T07:58:43.024-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="4.5 GiB"
time=2026-02-11T07:58:43.024-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-11T07:58:45.223-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35449"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T07:58:45.648-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T07:58:45.648-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 34249"
time=2026-02-11T07:58:45.649-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.4 GiB" free_swap="3.6 GiB"
time=2026-02-11T07:58:45.649-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T07:58:45.649-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T07:58:45.650-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T07:58:45.650-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T07:58:45.650-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T07:58:45.650-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T07:58:45.670-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T07:58:45.738-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T07:58:45.744-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:34249"
time=2026-02-11T07:58:45.749-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T07:58:45.749-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:58:45.750-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4630452
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4521 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-11T07:59:09.518-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.87 seconds"
time=2026-02-11T07:59:09.537-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-11T07:59:09.538-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:59:09.539-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.89 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/11 - 07:59:12 | 200 | 27.247780539s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T07:59:39.813-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36715"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T07:59:41.207-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T07:59:41.208-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 46071"
time=2026-02-11T07:59:41.208-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.4 GiB" free_swap="3.6 GiB"
time=2026-02-11T07:59:41.208-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T07:59:41.208-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T07:59:41.209-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T07:59:41.209-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T07:59:41.209-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T07:59:41.209-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T07:59:41.230-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T07:59:41.297-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T07:59:41.303-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:46071"
time=2026-02-11T07:59:41.308-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T07:59:41.308-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:59:41.309-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4640392
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4531 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-11T07:59:46.857-05:00 level=INFO source=server.go:1388 msg="llama runner started in 5.65 seconds"
time=2026-02-11T07:59:46.857-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-11T07:59:46.857-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T07:59:46.858-05:00 level=INFO source=server.go:1388 msg="llama runner started in 5.65 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/11 - 07:59:49 | 200 |  9.363928724s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T08:00:06.323-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42143"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T08:00:06.802-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T08:00:06.803-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 41443"
time=2026-02-11T08:00:06.803-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.4 GiB" free_swap="3.6 GiB"
time=2026-02-11T08:00:06.803-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.0 GiB" free="4.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T08:00:06.803-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T08:00:06.804-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T08:00:06.804-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T08:00:06.804-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T08:00:06.804-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T08:00:06.824-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T08:00:06.891-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T08:00:06.897-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:41443"
time=2026-02-11T08:00:06.904-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T08:00:06.904-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:00:06.905-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4617088
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4508 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-11T08:00:13.465-05:00 level=INFO source=server.go:1388 msg="llama runner started in 6.66 seconds"
time=2026-02-11T08:00:13.466-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-11T08:00:13.466-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:00:13.467-05:00 level=INFO source=server.go:1388 msg="llama runner started in 6.66 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/11 - 08:00:15 | 200 |  9.396254598s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T08:00:44.482-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40383"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T08:00:44.956-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T08:00:44.957-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 34683"
time=2026-02-11T08:00:44.958-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.4 GiB" free_swap="3.6 GiB"
time=2026-02-11T08:00:44.958-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.9 GiB" free="4.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T08:00:44.958-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T08:00:44.959-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T08:00:44.959-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T08:00:44.959-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T08:00:44.959-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T08:00:44.983-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T08:00:45.051-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T08:00:45.056-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:34683"
time=2026-02-11T08:00:45.058-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T08:00:45.059-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:00:45.060-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4591900
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4484 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-11T08:00:48.643-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.69 seconds"
time=2026-02-11T08:00:48.643-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-11T08:00:48.662-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:00:48.663-05:00 level=INFO source=server.go:1388 msg="llama runner started in 3.70 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/11 - 08:00:50 | 200 |  6.437255688s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-11T08:02:32.578-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39785"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-11T08:02:33.523-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-11T08:02:33.524-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 44497"
time=2026-02-11T08:02:33.524-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.3 GiB" free_swap="3.6 GiB"
time=2026-02-11T08:02:33.524-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.9 GiB" free="4.3 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-11T08:02:33.524-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-11T08:02:33.525-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-11T08:02:33.525-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-11T08:02:33.525-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-11T08:02:33.525-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-11T08:02:33.546-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-11T08:02:33.614-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-11T08:02:33.619-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:44497"
time=2026-02-11T08:02:33.624-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-11T08:02:33.625-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:02:33.626-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4547292
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4440 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-11T08:02:39.168-05:00 level=INFO source=server.go:1388 msg="llama runner started in 5.64 seconds"
time=2026-02-11T08:02:39.169-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-11T08:02:39.170-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-11T08:02:39.171-05:00 level=INFO source=server.go:1388 msg="llama runner started in 5.65 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/11 - 08:02:41 | 200 |  8.863208693s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-13T23:39:13.953-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36521"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-13T23:39:14.969-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-13T23:39:14.970-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 33635"
time=2026-02-13T23:39:14.971-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="3.8 GiB" free_swap="3.6 GiB"
time=2026-02-13T23:39:14.971-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.3 GiB" free="3.8 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-13T23:39:14.971-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-13T23:39:14.972-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-13T23:39:14.972-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-13T23:39:14.972-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-13T23:39:14.972-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-13T23:39:14.997-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-13T23:39:15.081-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-13T23:39:15.087-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:33635"
time=2026-02-13T23:39:15.093-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-13T23:39:15.094-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:39:15.094-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3919256
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 3827 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 732.30 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 767877120
llama_model_load: error loading model: unable to allocate CUDA0 buffer
llama_model_load_from_file_impl: failed to load model
panic: unable to load model: /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b

goroutine 39 [running]:
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x4000536320, {{0x4000615380, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x4000615378, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:843 +0x2a4
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 36
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc
time=2026-02-13T23:39:15.851-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
time=2026-02-13T23:39:15.884-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-13T23:39:16.103-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer\nllama_model_load_from_file_impl: failed to load model"
[GIN] 2026/02/13 - 23:39:16 | 500 |  2.405005924s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-13T23:39:35.223-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-13T23:39:35.437-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-13T23:39:35.438-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-13T23:39:35.440-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-13T23:39:35.554-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-13T23:39:35.582-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40669"
time=2026-02-13T23:39:39.539-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45195"
time=2026-02-13T23:39:39.751-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="4.0 GiB"
time=2026-02-13T23:39:39.751-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-13T23:39:42.102-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35559"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-13T23:39:42.579-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-13T23:39:42.580-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 36215"
time=2026-02-13T23:39:42.580-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.0 GiB" free_swap="3.6 GiB"
time=2026-02-13T23:39:42.581-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.5 GiB" free="4.0 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-13T23:39:42.581-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-13T23:39:42.581-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-13T23:39:42.581-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-13T23:39:42.581-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-13T23:39:42.581-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-13T23:39:42.604-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-13T23:39:42.673-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-13T23:39:42.678-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:36215"
time=2026-02-13T23:39:42.680-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-13T23:39:42.681-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:39:42.681-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4168960
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4071 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-13T23:40:06.805-05:00 level=INFO source=server.go:1388 msg="llama runner started in 24.22 seconds"
time=2026-02-13T23:40:06.827-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-13T23:40:06.827-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:40:06.830-05:00 level=INFO source=server.go:1388 msg="llama runner started in 24.25 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/13 - 23:40:09 | 200 | 27.552087606s |       127.0.0.1 | POST     "/api/generate"
time=2026-02-13T23:50:43.530-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33803"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-13T23:50:44.919-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-13T23:50:44.920-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 46373"
time=2026-02-13T23:50:44.920-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.1 GiB" free_swap="3.6 GiB"
time=2026-02-13T23:50:44.921-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="3.7 GiB" free="4.1 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-13T23:50:44.921-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-13T23:50:44.921-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-13T23:50:44.921-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-13T23:50:44.921-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-13T23:50:44.921-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-13T23:50:44.942-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-13T23:50:45.013-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-13T23:50:45.018-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:46373"
time=2026-02-13T23:50:45.020-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-13T23:50:45.021-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:50:45.021-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4289300
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4188 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 384.00 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 402653184
llama_init_from_model: failed to initialize the context: failed to allocate buffer for kv cache
panic: unable to create llama context

goroutine 9 [running]:
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x40004f66e0, {{0x4000687aa0, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x4000687a98, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:849 +0x298
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 35
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc
time=2026-02-13T23:50:47.528-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-13T23:50:47.539-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: cudaMalloc failed: out of memory"
[GIN] 2026/02/13 - 23:50:47 | 500 |   4.19506722s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-13T23:51:04.991-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-13T23:51:05.141-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-13T23:51:05.142-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-13T23:51:05.143-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-13T23:51:05.257-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-13T23:51:05.314-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45365"
time=2026-02-13T23:51:09.634-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44583"
time=2026-02-13T23:51:09.860-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="4.6 GiB"
time=2026-02-13T23:51:09.860-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-13T23:51:12.188-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33381"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-13T23:51:12.694-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-13T23:51:12.695-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 43253"
time=2026-02-13T23:51:12.695-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="4.6 GiB" free_swap="3.6 GiB"
time=2026-02-13T23:51:12.695-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="4.1 GiB" free="4.6 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-13T23:51:12.695-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-13T23:51:12.696-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-13T23:51:12.696-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-13T23:51:12.696-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-13T23:51:12.696-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-13T23:51:12.716-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-13T23:51:12.783-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-13T23:51:12.789-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:43253"
time=2026-02-13T23:51:12.797-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-13T23:51:12.798-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:51:12.798-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 4763260
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 4651 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
load_tensors: loaded 457 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f
warmup: warmup with image size = 378 x 378
alloc_compute_meta:      CUDA0 compute buffer size =    30.83 MiB
alloc_compute_meta:        CPU compute buffer size =     1.64 MiB
alloc_compute_meta: graph splits = 1, nodes = 860
warmup: flash attention is enabled
time=2026-02-13T23:51:36.369-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.67 seconds"
time=2026-02-13T23:51:36.370-05:00 level=INFO source=sched.go:537 msg="loaded runners" count=1
time=2026-02-13T23:51:36.370-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-13T23:51:36.371-05:00 level=INFO source=server.go:1388 msg="llama runner started in 23.68 seconds"
image_tokens->nx = 729
image_tokens->ny = 1
batch_f32 size = 1
[GIN] 2026/02/13 - 23:51:38 | 200 | 26.876871192s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-15T00:14:36.582-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-15T00:14:37.046-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-15T00:14:37.047-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-15T00:14:37.049-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-15T00:14:37.108-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-15T00:14:37.306-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42449"
time=2026-02-15T00:14:38.726-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37407"
time=2026-02-15T00:14:38.937-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="6.7 GiB"
time=2026-02-15T00:14:38.937-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-15T12:53:55.538-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38481"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-15T12:53:56.627-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-15T12:53:56.627-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 46497"
time=2026-02-15T12:53:56.628-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="2.9 GiB" free_swap="3.7 GiB"
time=2026-02-15T12:53:56.628-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="2.4 GiB" free="2.9 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-15T12:53:56.628-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-15T12:53:56.629-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-15T12:53:56.629-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-15T12:53:56.629-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-15T12:53:56.629-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-15T12:53:56.647-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-15T12:53:56.860-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-15T12:53:56.874-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:46497"
time=2026-02-15T12:53:56.878-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-15T12:53:56.878-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-15T12:53:56.879-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 2958484
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 2889 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 732.30 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 767877120
llama_model_load: error loading model: unable to allocate CUDA0 buffer
llama_model_load_from_file_impl: failed to load model
panic: unable to load model: /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b

goroutine 23 [running]:
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x400034e6e0, {{0x4000516f10, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x4000516f08, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:843 +0x2a4
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 10
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc
time=2026-02-15T12:54:06.910-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-15T12:54:06.987-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer"
[GIN] 2026/02/15 - 12:54:07 | 500 |  11.84604566s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-15T12:54:18.707-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-15T12:54:18.719-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-15T12:54:18.720-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-15T12:54:18.721-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-15T12:54:18.724-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-15T12:54:18.726-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39595"
time=2026-02-15T12:54:21.202-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39641"
time=2026-02-15T12:54:21.433-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="3.0 GiB"
time=2026-02-15T12:54:21.433-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-15T12:54:23.750-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45867"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-15T12:54:24.216-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-15T12:54:24.217-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 43453"
time=2026-02-15T12:54:24.218-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="2.9 GiB" free_swap="3.7 GiB"
time=2026-02-15T12:54:24.218-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="2.5 GiB" free="2.9 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-15T12:54:24.219-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-15T12:54:24.220-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-15T12:54:24.220-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-15T12:54:24.220-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-15T12:54:24.220-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-15T12:54:24.245-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-15T12:54:24.311-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-15T12:54:24.316-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:43453"
time=2026-02-15T12:54:24.319-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-15T12:54:24.320-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-15T12:54:24.320-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3059856
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 2988 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 867.61 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 909753600
ggml-backend.cpp:199: GGML_ASSERT(buffer) failed
[New LWP 10390]
[New LWP 10391]
[New LWP 10392]
[New LWP 10393]
[New LWP 10394]
[New LWP 10395]
[New LWP 10396]
[New LWP 10397]
[New LWP 10400]
[New LWP 10431]
time=2026-02-15T12:54:36.886-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
0x0000aaaae0ff116c in ?? ()
#0  0x0000aaaae0ff116c in ?? ()
#1  0x0000000000000080 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
[Inferior 1 (process 10389) detached]
SIGABRT: abort
PC=0xffff8bae2008 m=5 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 10 gp=0x4000502c40 m=5 mp=0x4000100008 [syscall]:
runtime.cgocall(0xaaaae1c55fbc, 0x4000531c08)
	runtime/cgocall.go:167 +0x44 fp=0x4000531bd0 sp=0x4000531b90 pc=0xaaaae0fe4914
github.com/ollama/ollama/llama._Cfunc_mtmd_init_from_file(0xffff24000bf0, 0xffff24000c70, {0x1, 0x1, 0x4, 0xaaaae2186eef, 0xaaaae2186ee3, 0xffffffff, 0x1, 0xffffffff, ...})
	_cgo_gotypes.go:1134 +0x34 fp=0x4000531c00 sp=0x4000531bd0 pc=0xaaaae139e4b4
github.com/ollama/ollama/llama.NewMtmdContext.func2(...)
	github.com/ollama/ollama/llama/llama.go:539
github.com/ollama/ollama/llama.NewMtmdContext(0x4000519bf0, {0x40000d0150, 0x70})
	github.com/ollama/ollama/llama/llama.go:539 +0x12c fp=0x4000531d20 sp=0x4000531c00 pc=0xaaaae13a1c0c
github.com/ollama/ollama/runner/llamarunner.NewImageContext(0x4000519bf0, {0x40000d0150, 0x70})
	github.com/ollama/ollama/runner/llamarunner/image.go:35 +0xe8 fp=0x4000531da0 sp=0x4000531d20 pc=0xaaaae1441328
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x4000096320, {{0x4000614a20, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x4000614a18, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:861 +0x1d8 fp=0x4000531ef0 sp=0x4000531da0 pc=0xaaaae1445998
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0xb0 fp=0x4000531fd0 sp=0x4000531ef0 pc=0xaaaae1446730
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000531fd0 sp=0x4000531fd0 pc=0xaaaae0fefd14
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 7
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc

goroutine 1 gp=0x40000021c0 m=nil [IO wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40003e1710 sp=0x40003e16f0 pc=0xaaaae0fe7e28
runtime.netpollblock(0x7000000000?, 0x6?, 0x0?)
	runtime/netpoll.go:575 +0x158 fp=0x40003e1750 sp=0x40003e1710 pc=0xaaaae0facde8
internal/poll.runtime_pollWait(0xffff44777de0, 0x72)
	runtime/netpoll.go:351 +0xa0 fp=0x40003e1780 sp=0x40003e1750 pc=0xaaaae0fe6fe0
internal/poll.(*pollDesc).wait(0x40001deb80?, 0xaaaae0f8ed8c?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x40003e17b0 sp=0x40003e1780 pc=0xaaaae10695f8
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0x40001deb80)
	internal/poll/fd_unix.go:620 +0x24c fp=0x40003e1860 sp=0x40003e17b0 pc=0xaaaae106decc
net.(*netFD).accept(0x40001deb80)
	net/fd_unix.go:172 +0x28 fp=0x40003e1920 sp=0x40003e1860 pc=0xaaaae10dc9d8
net.(*TCPListener).accept(0x4000115a00)
	net/tcpsock_posix.go:159 +0x24 fp=0x40003e1970 sp=0x40003e1920 pc=0xaaaae10f1e74
net.(*TCPListener).Accept(0x4000115a00)
	net/tcpsock.go:380 +0x2c fp=0x40003e19b0 sp=0x40003e1970 pc=0xaaaae10f0e0c
net/http.(*onceCloseListener).Accept(0x40000bc1b0?)
	<autogenerated>:1 +0x30 fp=0x40003e19d0 sp=0x40003e19b0 pc=0xaaaae12cb360
net/http.(*Server).Serve(0x40000b4000, {0xaaaae24d9da0, 0x4000115a00})
	net/http/server.go:3424 +0x290 fp=0x40003e1b00 sp=0x40003e19d0 pc=0xaaaae12a4aa0
github.com/ollama/ollama/runner/llamarunner.Execute({0x4000116200, 0x4, 0x4})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1002 +0x7ac fp=0x40003e1cd0 sp=0x40003e1b00 pc=0xaaaae1446fcc
github.com/ollama/ollama/runner.Execute({0x40001161f0?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0x16c fp=0x40003e1d10 sp=0x40003e1cd0 pc=0xaaaae1539a1c
github.com/ollama/ollama/cmd.NewCLI.func3(0x40001ff700?, {0xaaaae1f510fd?, 0x4?, 0xaaaae1f51101?})
	github.com/ollama/ollama/cmd/cmd.go:1979 +0x54 fp=0x40003e1d40 sp=0x40003e1d10 pc=0xaaaae1bf6924
github.com/spf13/cobra.(*Command).execute(0x40004e5508, {0x400061af40, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x40003e1e60 sp=0x40003e1d40 pc=0xaaaae114c6d8
github.com/spf13/cobra.(*Command).ExecuteC(0x40004be908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x40003e1f20 sp=0x40003e1e60 pc=0xaaaae114ce20
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x54 fp=0x40003e1f40 sp=0x40003e1f20 pc=0xaaaae1bf7474
runtime.main()
	runtime/proc.go:283 +0x284 fp=0x40003e1fd0 sp=0x40003e1f40 pc=0xaaaae0fb4194
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40003e1fd0 sp=0x40003e1fd0 pc=0xaaaae0fefd14

goroutine 2 gp=0x4000002c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ef90 sp=0x400005ef70 pc=0xaaaae0fe7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0x400005efd0 sp=0x400005ef90 pc=0xaaaae0fb44e8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005efd0 sp=0x400005efd0 pc=0xaaaae0fefd14
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x24

goroutine 18 gp=0x4000102380 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005a760 sp=0x400005a740 pc=0xaaaae0fe7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0x4000110000)
	runtime/mgcsweep.go:316 +0x108 fp=0x400005a7b0 sp=0x400005a760 pc=0xaaaae0f9ed18
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x28 fp=0x400005a7d0 sp=0x400005a7b0 pc=0xaaaae0f92b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005a7d0 sp=0x400005a7d0 pc=0xaaaae0fefd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x6c

goroutine 19 gp=0x4000102540 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xaaaae213e828?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005af60 sp=0x400005af40 pc=0xaaaae0fe7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0xaaaae2de2580)
	runtime/mgcscavenge.go:425 +0x5c fp=0x400005af90 sp=0x400005af60 pc=0xaaaae0f9c7dc
runtime.bgscavenge(0x4000110000)
	runtime/mgcscavenge.go:658 +0xac fp=0x400005afb0 sp=0x400005af90 pc=0xaaaae0f9cd5c
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x28 fp=0x400005afd0 sp=0x400005afb0 pc=0xaaaae0f92ae8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005afd0 sp=0x400005afd0 pc=0xaaaae0fefd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xac

goroutine 20 gp=0x4000102a80 m=nil [finalizer wait]:
runtime.gopark(0x0?, 0xaaaae24c51f8?, 0x60?, 0x40?, 0x1000000010?)
	runtime/proc.go:435 +0xc8 fp=0x400005e590 sp=0x400005e570 pc=0xaaaae0fe7e28
runtime.runfinq()
	runtime/mfinal.go:196 +0x108 fp=0x400005e7d0 sp=0x400005e590 pc=0xaaaae0f91b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005e7d0 sp=0x400005e7d0 pc=0xaaaae0fefd14
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x80

goroutine 21 gp=0x4000103500 m=nil [chan receive]:
runtime.gopark(0x40002377c0?, 0x400000e5a0?, 0x48?, 0xb7?, 0xaaaae10b4f98?)
	runtime/proc.go:435 +0xc8 fp=0x400005b6f0 sp=0x400005b6d0 pc=0xaaaae0fe7e28
runtime.chanrecv(0x4000118310, 0x0, 0x1)
	runtime/chan.go:664 +0x42c fp=0x400005b770 sp=0x400005b6f0 pc=0xaaaae0f83bac
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x14 fp=0x400005b7a0 sp=0x400005b770 pc=0xaaaae0f83744
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x3c fp=0x400005b7d0 sp=0x400005b7a0 pc=0xaaaae0f95d6c
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005b7d0 sp=0x400005b7d0 pc=0xaaaae0fefd14
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x78

goroutine 22 gp=0x400049a1c0 m=nil [GC worker (idle)]:
runtime.gopark(0x2980033dfca8?, 0x3?, 0x63?, 0x4e?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005bf10 sp=0x400005bef0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005bfb0 sp=0x400005bf10 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005bfd0 sp=0x400005bfb0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005bfd0 sp=0x400005bfd0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 3 gp=0x4000003500 m=nil [GC worker (idle)]:
runtime.gopark(0x29800333d477?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005f710 sp=0x400005f6f0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005f7b0 sp=0x400005f710 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005f7d0 sp=0x400005f7b0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005f7d0 sp=0x400005f7d0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 4 gp=0x40000036c0 m=nil [GC worker (idle)]:
runtime.gopark(0x298003e58bfc?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ff10 sp=0x400005fef0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005ffb0 sp=0x400005ff10 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005ffd0 sp=0x400005ffb0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005ffd0 sp=0x400005ffd0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 5 gp=0x4000003880 m=nil [GC worker (idle)]:
runtime.gopark(0xaaaae2eb8100?, 0x2?, 0xf6?, 0x11?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000060710 sp=0x40000606f0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x40000607b0 sp=0x4000060710 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40000607d0 sp=0x40000607b0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000607d0 sp=0x40000607d0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 23 gp=0x400049a380 m=nil [GC worker (idle)]:
runtime.gopark(0x2980033e1028?, 0x3?, 0xa1?, 0x68?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005c710 sp=0x400005c6f0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400005c7b0 sp=0x400005c710 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005c7d0 sp=0x400005c7b0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005c7d0 sp=0x400005c7d0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 24 gp=0x400049a540 m=nil [GC worker (idle)]:
runtime.gopark(0x2980033e0c48?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400052ef10 sp=0x400052eef0 pc=0xaaaae0fe7e28
runtime.gcBgMarkWorker(0x4000119730)
	runtime/mgc.go:1423 +0xdc fp=0x400052efb0 sp=0x400052ef10 pc=0xaaaae0f94fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400052efd0 sp=0x400052efb0 pc=0xaaaae0f94ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400052efd0 sp=0x400052efd0 pc=0xaaaae0fefd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 6 gp=0x4000502700 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0xaaaae2df1f00?, 0x0?, 0x60?, 0x20?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x40000a3620 sp=0x40000a3600 pc=0xaaaae0fe7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0x4000096340, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x204 fp=0x40000a3670 sp=0x40000a3620 pc=0xaaaae0fc8634
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x2c fp=0x40000a36b0 sp=0x40000a3670 pc=0xaaaae0fe97dc
sync.(*WaitGroup).Wait(0x4000096338)
	sync/waitgroup.go:118 +0x70 fp=0x40000a36d0 sp=0x40000a36b0 pc=0xaaaae0ffb3f0
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0x4000096320, {0xaaaae24dc480, 0x400009a140})
	github.com/ollama/ollama/runner/llamarunner/runner.go:360 +0x40 fp=0x40000a37a0 sp=0x40000a36d0 pc=0xaaaae1442bc0
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x30 fp=0x40000a37d0 sp=0x40000a37a0 pc=0xaaaae14471f0
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000a37d0 sp=0x40000a37d0 pc=0xaaaae0fefd14
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x44c

r0      0x0
r1      0x2899
r2      0x6
r3      0xffff3f7ef0c0
r4      0xffff8bffcb58
r5      0xffff3f7ed7f0
r6      0xfffffff8
r7      0xffff3f7ed7d0
r8      0x83
r9      0x0
r10     0xffff8ba64860
r11     0x0
r12     0x746977735f747874
r13     0x0
r14     0x3120363131203430
r15     0x3120353331203032
r16     0x1
r17     0xaaaae2d20988
r18     0x0
r19     0x2899
r20     0xffff3f7ef0c0
r21     0x6
r22     0x1c
r23     0xaaaae2182954
r24     0xaaaae217d981
r25     0xffff3f7ee248
r26     0x1a40
r27     0xffff28001bd0
r28     0xaaaae2182968
r29     0xffff3f7ed6e0
lr      0xffff8bae1ff4
sp      0xffff3f7ed650
pc      0xffff8bae2008
fault   0x0
time=2026-02-15T12:54:37.642-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server error"
time=2026-02-15T12:54:37.772-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-15T12:54:37.893-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: GGML_ASSERT(buffer) failed"
[GIN] 2026/02/15 - 12:54:38 | 500 | 14.481265523s |       127.0.0.1 | POST     "/api/generate"
sudo: /usr/bin/nvpmodel: command not found
3
ðŸš€ Starting Chitti Brain (Ollama) in Privacy-First mode...
time=2026-02-15T12:54:52.709-05:00 level=INFO source=routes.go:1636 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:0 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/rameshthiyagu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-02-15T12:54:52.723-05:00 level=INFO source=images.go:473 msg="total blobs: 12"
time=2026-02-15T12:54:52.724-05:00 level=INFO source=images.go:480 msg="total unused blobs removed: 0"
time=2026-02-15T12:54:52.725-05:00 level=INFO source=routes.go:1689 msg="Listening on 127.0.0.1:11434 (version 0.15.6)"
time=2026-02-15T12:54:52.728-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-02-15T12:54:52.730-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38129"
time=2026-02-15T12:54:55.152-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33981"
time=2026-02-15T12:54:55.377-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b filter_id="" library=CUDA compute=8.7 name=CUDA0 description=Orin libdirs=ollama,cuda_jetpack6 driver=12.6 pci_id=0000:00:00.0 type=iGPU total="7.4 GiB" available="3.0 GiB"
time=2026-02-15T12:54:55.377-05:00 level=INFO source=routes.go:1739 msg="vram-based default context" total_vram="7.4 GiB" default_num_ctx=4096
time=2026-02-15T12:54:57.692-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37157"
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-02-15T12:54:58.189-05:00 level=WARN source=server.go:169 msg="requested context size too large for model" num_ctx=4096 n_ctx_train=2048
time=2026-02-15T12:54:58.190-05:00 level=INFO source=server.go:431 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b --port 45317"
time=2026-02-15T12:54:58.191-05:00 level=INFO source=sched.go:463 msg="system memory" total="7.4 GiB" free="3.0 GiB" free_swap="3.7 GiB"
time=2026-02-15T12:54:58.191-05:00 level=INFO source=sched.go:470 msg="gpu memory" id=GPU-a8527b81-cde0-57b8-9407-c59125c7500b library=CUDA available="2.5 GiB" free="3.0 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-02-15T12:54:58.191-05:00 level=INFO source=server.go:498 msg="loading model" "model layers"=25 requested=-1
time=2026-02-15T12:54:58.191-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="732.3 MiB"
time=2026-02-15T12:54:58.192-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="384.0 MiB"
time=2026-02-15T12:54:58.192-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="148.0 MiB"
time=2026-02-15T12:54:58.192-05:00 level=INFO source=device.go:272 msg="total memory" size="1.2 GiB"
time=2026-02-15T12:54:58.213-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes, ID: GPU-a8527b81-cde0-57b8-9407-c59125c7500b
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_jetpack6/libggml-cuda.so
time=2026-02-15T12:54:58.278-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.LLAMAFILE=1 CPU.1.NEON=1 CPU.1.ARM_FMA=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=870 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2026-02-15T12:54:58.284-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:45317"
time=2026-02-15T12:54:58.291-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:2048 KvCacheType: NumThreads:6 GPULayers:25[ID:GPU-a8527b81-cde0-57b8-9407-c59125c7500b Layers:25(0..24)] MultiUserCache:false ProjectorPath:/home/rameshthiyagu/.ollama/models/blobs/sha256-4cc1cb3660d87ff56432ebeb7884ad35d67c48c7b9f6b2856f305e39c38eed8f MainGPU:0 UseMmap:true}"
time=2026-02-15T12:54:58.292-05:00 level=INFO source=server.go:1350 msg="waiting for llama runner to start responding"
time=2026-02-15T12:54:58.292-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_get_available_uma_memory: final available_memory_kb: 3076444
llama_model_load_from_file_impl: using device CUDA0 (Orin) (0000:00:00.0) - 3004 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 245 tensors from /home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi2
llama_model_loader: - kv   1:                               general.name str              = moondream2
llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048
llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048
llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi2.block_count u32              = 24
llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = ["Ä  t", "Ä  a", "h e", "i n", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  147 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 788.55 MiB (4.66 BPW) 
load: missing or unrecognized pre-tokenizer type, using: 'default'
load: printing all EOG tokens:
load:   - 50256 ('<|endoftext|>')
load: special tokens cache size = 944
load: token to piece cache size = 0.3151 MB
print_info: arch             = phi2
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 24
print_info: n_head           = 32
print_info: n_head_kv        = 32
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 1B
print_info: model params     = 1.42 B
print_info: general.name     = moondream2
print_info: vocab type       = BPE
print_info: n_vocab          = 51200
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: UNK token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    56.25 MiB
load_tensors:        CUDA0 model buffer size =   732.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_seq     = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.20 MiB
llama_kv_cache:      CUDA0 KV buffer size =   384.00 MiB
llama_kv_cache: size =  384.00 MiB (  2048 cells,  24 layers,  1/1 seqs), K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   104.00 MiB
llama_context:  CUDA_Host compute buffer size =     8.01 MiB
llama_context: graph nodes  = 778
llama_context: graph splits = 2
clip_model_loader: model name:   vikhyatk/moondream2
clip_model_loader: description:  image encoder for vikhyatk/moondream2
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    457
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
clip_model_loader: tensor[0]: n_dims = 2, name = mm.0.weight, tensor_size=18874368, offset=0, shape:[1152, 8192, 1, 1], type = f16
clip_model_loader: tensor[1]: n_dims = 1, name = mm.0.bias, tensor_size=32768, offset=18874368, shape:[8192, 1, 1, 1], type = f32
clip_model_loader: tensor[2]: n_dims = 2, name = mm.2.weight, tensor_size=33554432, offset=18907136, shape:[8192, 2048, 1, 1], type = f16
clip_model_loader: tensor[3]: n_dims = 1, name = mm.2.bias, tensor_size=8192, offset=52461568, shape:[2048, 1, 1, 1], type = f32
clip_model_loader: tensor[4]: n_dims = 2, name = v.position_embd.weight, tensor_size=1679616, offset=52469760, shape:[1152, 729, 1, 1], type = f16
clip_model_loader: tensor[5]: n_dims = 4, name = v.patch_embd.weight, tensor_size=1354752, offset=54149376, shape:[14, 14, 3, 1152], type = f16
clip_model_loader: tensor[6]: n_dims = 1, name = v.patch_embd.bias, tensor_size=4608, offset=55504128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[7]: n_dims = 1, name = v.post_ln.weight, tensor_size=4608, offset=55508736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[8]: n_dims = 1, name = v.post_ln.bias, tensor_size=4608, offset=55513344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[9]: n_dims = 2, name = v.blk.0.attn_q.weight, tensor_size=2654208, offset=55517952, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[10]: n_dims = 1, name = v.blk.0.attn_q.bias, tensor_size=4608, offset=58172160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[11]: n_dims = 2, name = v.blk.0.attn_k.weight, tensor_size=2654208, offset=58176768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[12]: n_dims = 1, name = v.blk.0.attn_k.bias, tensor_size=4608, offset=60830976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[13]: n_dims = 2, name = v.blk.0.attn_v.weight, tensor_size=2654208, offset=60835584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[14]: n_dims = 1, name = v.blk.0.attn_v.bias, tensor_size=4608, offset=63489792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[15]: n_dims = 2, name = v.blk.0.attn_out.weight, tensor_size=2654208, offset=63494400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[16]: n_dims = 1, name = v.blk.0.attn_out.bias, tensor_size=4608, offset=66148608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[17]: n_dims = 1, name = v.blk.0.ln1.weight, tensor_size=4608, offset=66153216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[18]: n_dims = 1, name = v.blk.0.ln1.bias, tensor_size=4608, offset=66157824, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[19]: n_dims = 2, name = v.blk.0.ffn_down.weight, tensor_size=9916416, offset=66162432, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[20]: n_dims = 1, name = v.blk.0.ffn_down.bias, tensor_size=17216, offset=76078848, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[21]: n_dims = 2, name = v.blk.0.ffn_up.weight, tensor_size=9916416, offset=76096064, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[22]: n_dims = 1, name = v.blk.0.ffn_up.bias, tensor_size=4608, offset=86012480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[23]: n_dims = 1, name = v.blk.0.ln2.weight, tensor_size=4608, offset=86017088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[24]: n_dims = 1, name = v.blk.0.ln2.bias, tensor_size=4608, offset=86021696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[25]: n_dims = 2, name = v.blk.1.attn_q.weight, tensor_size=2654208, offset=86026304, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[26]: n_dims = 1, name = v.blk.1.attn_q.bias, tensor_size=4608, offset=88680512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[27]: n_dims = 2, name = v.blk.1.attn_k.weight, tensor_size=2654208, offset=88685120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[28]: n_dims = 1, name = v.blk.1.attn_k.bias, tensor_size=4608, offset=91339328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[29]: n_dims = 2, name = v.blk.1.attn_v.weight, tensor_size=2654208, offset=91343936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[30]: n_dims = 1, name = v.blk.1.attn_v.bias, tensor_size=4608, offset=93998144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[31]: n_dims = 2, name = v.blk.1.attn_out.weight, tensor_size=2654208, offset=94002752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[32]: n_dims = 1, name = v.blk.1.attn_out.bias, tensor_size=4608, offset=96656960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[33]: n_dims = 1, name = v.blk.1.ln1.weight, tensor_size=4608, offset=96661568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[34]: n_dims = 1, name = v.blk.1.ln1.bias, tensor_size=4608, offset=96666176, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[35]: n_dims = 2, name = v.blk.1.ffn_down.weight, tensor_size=9916416, offset=96670784, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[36]: n_dims = 1, name = v.blk.1.ffn_down.bias, tensor_size=17216, offset=106587200, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[37]: n_dims = 2, name = v.blk.1.ffn_up.weight, tensor_size=9916416, offset=106604416, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[38]: n_dims = 1, name = v.blk.1.ffn_up.bias, tensor_size=4608, offset=116520832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[39]: n_dims = 1, name = v.blk.1.ln2.weight, tensor_size=4608, offset=116525440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[40]: n_dims = 1, name = v.blk.1.ln2.bias, tensor_size=4608, offset=116530048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[41]: n_dims = 2, name = v.blk.2.attn_q.weight, tensor_size=2654208, offset=116534656, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[42]: n_dims = 1, name = v.blk.2.attn_q.bias, tensor_size=4608, offset=119188864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[43]: n_dims = 2, name = v.blk.2.attn_k.weight, tensor_size=2654208, offset=119193472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[44]: n_dims = 1, name = v.blk.2.attn_k.bias, tensor_size=4608, offset=121847680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[45]: n_dims = 2, name = v.blk.2.attn_v.weight, tensor_size=2654208, offset=121852288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[46]: n_dims = 1, name = v.blk.2.attn_v.bias, tensor_size=4608, offset=124506496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[47]: n_dims = 2, name = v.blk.2.attn_out.weight, tensor_size=2654208, offset=124511104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[48]: n_dims = 1, name = v.blk.2.attn_out.bias, tensor_size=4608, offset=127165312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[49]: n_dims = 1, name = v.blk.2.ln1.weight, tensor_size=4608, offset=127169920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[50]: n_dims = 1, name = v.blk.2.ln1.bias, tensor_size=4608, offset=127174528, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[51]: n_dims = 2, name = v.blk.2.ffn_down.weight, tensor_size=9916416, offset=127179136, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[52]: n_dims = 1, name = v.blk.2.ffn_down.bias, tensor_size=17216, offset=137095552, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[53]: n_dims = 2, name = v.blk.2.ffn_up.weight, tensor_size=9916416, offset=137112768, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[54]: n_dims = 1, name = v.blk.2.ffn_up.bias, tensor_size=4608, offset=147029184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[55]: n_dims = 1, name = v.blk.2.ln2.weight, tensor_size=4608, offset=147033792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[56]: n_dims = 1, name = v.blk.2.ln2.bias, tensor_size=4608, offset=147038400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[57]: n_dims = 2, name = v.blk.3.attn_q.weight, tensor_size=2654208, offset=147043008, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[58]: n_dims = 1, name = v.blk.3.attn_q.bias, tensor_size=4608, offset=149697216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[59]: n_dims = 2, name = v.blk.3.attn_k.weight, tensor_size=2654208, offset=149701824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[60]: n_dims = 1, name = v.blk.3.attn_k.bias, tensor_size=4608, offset=152356032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[61]: n_dims = 2, name = v.blk.3.attn_v.weight, tensor_size=2654208, offset=152360640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[62]: n_dims = 1, name = v.blk.3.attn_v.bias, tensor_size=4608, offset=155014848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[63]: n_dims = 2, name = v.blk.3.attn_out.weight, tensor_size=2654208, offset=155019456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[64]: n_dims = 1, name = v.blk.3.attn_out.bias, tensor_size=4608, offset=157673664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[65]: n_dims = 1, name = v.blk.3.ln1.weight, tensor_size=4608, offset=157678272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[66]: n_dims = 1, name = v.blk.3.ln1.bias, tensor_size=4608, offset=157682880, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[67]: n_dims = 2, name = v.blk.3.ffn_down.weight, tensor_size=9916416, offset=157687488, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[68]: n_dims = 1, name = v.blk.3.ffn_down.bias, tensor_size=17216, offset=167603904, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[69]: n_dims = 2, name = v.blk.3.ffn_up.weight, tensor_size=9916416, offset=167621120, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[70]: n_dims = 1, name = v.blk.3.ffn_up.bias, tensor_size=4608, offset=177537536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[71]: n_dims = 1, name = v.blk.3.ln2.weight, tensor_size=4608, offset=177542144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[72]: n_dims = 1, name = v.blk.3.ln2.bias, tensor_size=4608, offset=177546752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[73]: n_dims = 2, name = v.blk.4.attn_q.weight, tensor_size=2654208, offset=177551360, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[74]: n_dims = 1, name = v.blk.4.attn_q.bias, tensor_size=4608, offset=180205568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[75]: n_dims = 2, name = v.blk.4.attn_k.weight, tensor_size=2654208, offset=180210176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[76]: n_dims = 1, name = v.blk.4.attn_k.bias, tensor_size=4608, offset=182864384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[77]: n_dims = 2, name = v.blk.4.attn_v.weight, tensor_size=2654208, offset=182868992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[78]: n_dims = 1, name = v.blk.4.attn_v.bias, tensor_size=4608, offset=185523200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[79]: n_dims = 2, name = v.blk.4.attn_out.weight, tensor_size=2654208, offset=185527808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[80]: n_dims = 1, name = v.blk.4.attn_out.bias, tensor_size=4608, offset=188182016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[81]: n_dims = 1, name = v.blk.4.ln1.weight, tensor_size=4608, offset=188186624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[82]: n_dims = 1, name = v.blk.4.ln1.bias, tensor_size=4608, offset=188191232, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[83]: n_dims = 2, name = v.blk.4.ffn_down.weight, tensor_size=9916416, offset=188195840, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[84]: n_dims = 1, name = v.blk.4.ffn_down.bias, tensor_size=17216, offset=198112256, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[85]: n_dims = 2, name = v.blk.4.ffn_up.weight, tensor_size=9916416, offset=198129472, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[86]: n_dims = 1, name = v.blk.4.ffn_up.bias, tensor_size=4608, offset=208045888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[87]: n_dims = 1, name = v.blk.4.ln2.weight, tensor_size=4608, offset=208050496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[88]: n_dims = 1, name = v.blk.4.ln2.bias, tensor_size=4608, offset=208055104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[89]: n_dims = 2, name = v.blk.5.attn_q.weight, tensor_size=2654208, offset=208059712, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[90]: n_dims = 1, name = v.blk.5.attn_q.bias, tensor_size=4608, offset=210713920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[91]: n_dims = 2, name = v.blk.5.attn_k.weight, tensor_size=2654208, offset=210718528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[92]: n_dims = 1, name = v.blk.5.attn_k.bias, tensor_size=4608, offset=213372736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[93]: n_dims = 2, name = v.blk.5.attn_v.weight, tensor_size=2654208, offset=213377344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[94]: n_dims = 1, name = v.blk.5.attn_v.bias, tensor_size=4608, offset=216031552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[95]: n_dims = 2, name = v.blk.5.attn_out.weight, tensor_size=2654208, offset=216036160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[96]: n_dims = 1, name = v.blk.5.attn_out.bias, tensor_size=4608, offset=218690368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[97]: n_dims = 1, name = v.blk.5.ln1.weight, tensor_size=4608, offset=218694976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[98]: n_dims = 1, name = v.blk.5.ln1.bias, tensor_size=4608, offset=218699584, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[99]: n_dims = 2, name = v.blk.5.ffn_down.weight, tensor_size=9916416, offset=218704192, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[100]: n_dims = 1, name = v.blk.5.ffn_down.bias, tensor_size=17216, offset=228620608, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[101]: n_dims = 2, name = v.blk.5.ffn_up.weight, tensor_size=9916416, offset=228637824, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[102]: n_dims = 1, name = v.blk.5.ffn_up.bias, tensor_size=4608, offset=238554240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[103]: n_dims = 1, name = v.blk.5.ln2.weight, tensor_size=4608, offset=238558848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[104]: n_dims = 1, name = v.blk.5.ln2.bias, tensor_size=4608, offset=238563456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[105]: n_dims = 2, name = v.blk.6.attn_q.weight, tensor_size=2654208, offset=238568064, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[106]: n_dims = 1, name = v.blk.6.attn_q.bias, tensor_size=4608, offset=241222272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[107]: n_dims = 2, name = v.blk.6.attn_k.weight, tensor_size=2654208, offset=241226880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[108]: n_dims = 1, name = v.blk.6.attn_k.bias, tensor_size=4608, offset=243881088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[109]: n_dims = 2, name = v.blk.6.attn_v.weight, tensor_size=2654208, offset=243885696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[110]: n_dims = 1, name = v.blk.6.attn_v.bias, tensor_size=4608, offset=246539904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[111]: n_dims = 2, name = v.blk.6.attn_out.weight, tensor_size=2654208, offset=246544512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[112]: n_dims = 1, name = v.blk.6.attn_out.bias, tensor_size=4608, offset=249198720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[113]: n_dims = 1, name = v.blk.6.ln1.weight, tensor_size=4608, offset=249203328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[114]: n_dims = 1, name = v.blk.6.ln1.bias, tensor_size=4608, offset=249207936, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[115]: n_dims = 2, name = v.blk.6.ffn_down.weight, tensor_size=9916416, offset=249212544, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[116]: n_dims = 1, name = v.blk.6.ffn_down.bias, tensor_size=17216, offset=259128960, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[117]: n_dims = 2, name = v.blk.6.ffn_up.weight, tensor_size=9916416, offset=259146176, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[118]: n_dims = 1, name = v.blk.6.ffn_up.bias, tensor_size=4608, offset=269062592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[119]: n_dims = 1, name = v.blk.6.ln2.weight, tensor_size=4608, offset=269067200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[120]: n_dims = 1, name = v.blk.6.ln2.bias, tensor_size=4608, offset=269071808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[121]: n_dims = 2, name = v.blk.7.attn_q.weight, tensor_size=2654208, offset=269076416, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[122]: n_dims = 1, name = v.blk.7.attn_q.bias, tensor_size=4608, offset=271730624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[123]: n_dims = 2, name = v.blk.7.attn_k.weight, tensor_size=2654208, offset=271735232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[124]: n_dims = 1, name = v.blk.7.attn_k.bias, tensor_size=4608, offset=274389440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[125]: n_dims = 2, name = v.blk.7.attn_v.weight, tensor_size=2654208, offset=274394048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[126]: n_dims = 1, name = v.blk.7.attn_v.bias, tensor_size=4608, offset=277048256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[127]: n_dims = 2, name = v.blk.7.attn_out.weight, tensor_size=2654208, offset=277052864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[128]: n_dims = 1, name = v.blk.7.attn_out.bias, tensor_size=4608, offset=279707072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[129]: n_dims = 1, name = v.blk.7.ln1.weight, tensor_size=4608, offset=279711680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[130]: n_dims = 1, name = v.blk.7.ln1.bias, tensor_size=4608, offset=279716288, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[131]: n_dims = 2, name = v.blk.7.ffn_down.weight, tensor_size=9916416, offset=279720896, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[132]: n_dims = 1, name = v.blk.7.ffn_down.bias, tensor_size=17216, offset=289637312, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[133]: n_dims = 2, name = v.blk.7.ffn_up.weight, tensor_size=9916416, offset=289654528, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[134]: n_dims = 1, name = v.blk.7.ffn_up.bias, tensor_size=4608, offset=299570944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[135]: n_dims = 1, name = v.blk.7.ln2.weight, tensor_size=4608, offset=299575552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[136]: n_dims = 1, name = v.blk.7.ln2.bias, tensor_size=4608, offset=299580160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[137]: n_dims = 2, name = v.blk.8.attn_q.weight, tensor_size=2654208, offset=299584768, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[138]: n_dims = 1, name = v.blk.8.attn_q.bias, tensor_size=4608, offset=302238976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[139]: n_dims = 2, name = v.blk.8.attn_k.weight, tensor_size=2654208, offset=302243584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[140]: n_dims = 1, name = v.blk.8.attn_k.bias, tensor_size=4608, offset=304897792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[141]: n_dims = 2, name = v.blk.8.attn_v.weight, tensor_size=2654208, offset=304902400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[142]: n_dims = 1, name = v.blk.8.attn_v.bias, tensor_size=4608, offset=307556608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[143]: n_dims = 2, name = v.blk.8.attn_out.weight, tensor_size=2654208, offset=307561216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[144]: n_dims = 1, name = v.blk.8.attn_out.bias, tensor_size=4608, offset=310215424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[145]: n_dims = 1, name = v.blk.8.ln1.weight, tensor_size=4608, offset=310220032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[146]: n_dims = 1, name = v.blk.8.ln1.bias, tensor_size=4608, offset=310224640, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[147]: n_dims = 2, name = v.blk.8.ffn_down.weight, tensor_size=9916416, offset=310229248, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[148]: n_dims = 1, name = v.blk.8.ffn_down.bias, tensor_size=17216, offset=320145664, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[149]: n_dims = 2, name = v.blk.8.ffn_up.weight, tensor_size=9916416, offset=320162880, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[150]: n_dims = 1, name = v.blk.8.ffn_up.bias, tensor_size=4608, offset=330079296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[151]: n_dims = 1, name = v.blk.8.ln2.weight, tensor_size=4608, offset=330083904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[152]: n_dims = 1, name = v.blk.8.ln2.bias, tensor_size=4608, offset=330088512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[153]: n_dims = 2, name = v.blk.9.attn_q.weight, tensor_size=2654208, offset=330093120, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[154]: n_dims = 1, name = v.blk.9.attn_q.bias, tensor_size=4608, offset=332747328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[155]: n_dims = 2, name = v.blk.9.attn_k.weight, tensor_size=2654208, offset=332751936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[156]: n_dims = 1, name = v.blk.9.attn_k.bias, tensor_size=4608, offset=335406144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[157]: n_dims = 2, name = v.blk.9.attn_v.weight, tensor_size=2654208, offset=335410752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[158]: n_dims = 1, name = v.blk.9.attn_v.bias, tensor_size=4608, offset=338064960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[159]: n_dims = 2, name = v.blk.9.attn_out.weight, tensor_size=2654208, offset=338069568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[160]: n_dims = 1, name = v.blk.9.attn_out.bias, tensor_size=4608, offset=340723776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[161]: n_dims = 1, name = v.blk.9.ln1.weight, tensor_size=4608, offset=340728384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[162]: n_dims = 1, name = v.blk.9.ln1.bias, tensor_size=4608, offset=340732992, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[163]: n_dims = 2, name = v.blk.9.ffn_down.weight, tensor_size=9916416, offset=340737600, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[164]: n_dims = 1, name = v.blk.9.ffn_down.bias, tensor_size=17216, offset=350654016, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[165]: n_dims = 2, name = v.blk.9.ffn_up.weight, tensor_size=9916416, offset=350671232, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[166]: n_dims = 1, name = v.blk.9.ffn_up.bias, tensor_size=4608, offset=360587648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[167]: n_dims = 1, name = v.blk.9.ln2.weight, tensor_size=4608, offset=360592256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[168]: n_dims = 1, name = v.blk.9.ln2.bias, tensor_size=4608, offset=360596864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[169]: n_dims = 2, name = v.blk.10.attn_q.weight, tensor_size=2654208, offset=360601472, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[170]: n_dims = 1, name = v.blk.10.attn_q.bias, tensor_size=4608, offset=363255680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[171]: n_dims = 2, name = v.blk.10.attn_k.weight, tensor_size=2654208, offset=363260288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[172]: n_dims = 1, name = v.blk.10.attn_k.bias, tensor_size=4608, offset=365914496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[173]: n_dims = 2, name = v.blk.10.attn_v.weight, tensor_size=2654208, offset=365919104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[174]: n_dims = 1, name = v.blk.10.attn_v.bias, tensor_size=4608, offset=368573312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[175]: n_dims = 2, name = v.blk.10.attn_out.weight, tensor_size=2654208, offset=368577920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[176]: n_dims = 1, name = v.blk.10.attn_out.bias, tensor_size=4608, offset=371232128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[177]: n_dims = 1, name = v.blk.10.ln1.weight, tensor_size=4608, offset=371236736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[178]: n_dims = 1, name = v.blk.10.ln1.bias, tensor_size=4608, offset=371241344, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[179]: n_dims = 2, name = v.blk.10.ffn_down.weight, tensor_size=9916416, offset=371245952, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[180]: n_dims = 1, name = v.blk.10.ffn_down.bias, tensor_size=17216, offset=381162368, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[181]: n_dims = 2, name = v.blk.10.ffn_up.weight, tensor_size=9916416, offset=381179584, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[182]: n_dims = 1, name = v.blk.10.ffn_up.bias, tensor_size=4608, offset=391096000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[183]: n_dims = 1, name = v.blk.10.ln2.weight, tensor_size=4608, offset=391100608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[184]: n_dims = 1, name = v.blk.10.ln2.bias, tensor_size=4608, offset=391105216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[185]: n_dims = 2, name = v.blk.11.attn_q.weight, tensor_size=2654208, offset=391109824, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[186]: n_dims = 1, name = v.blk.11.attn_q.bias, tensor_size=4608, offset=393764032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[187]: n_dims = 2, name = v.blk.11.attn_k.weight, tensor_size=2654208, offset=393768640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[188]: n_dims = 1, name = v.blk.11.attn_k.bias, tensor_size=4608, offset=396422848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[189]: n_dims = 2, name = v.blk.11.attn_v.weight, tensor_size=2654208, offset=396427456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[190]: n_dims = 1, name = v.blk.11.attn_v.bias, tensor_size=4608, offset=399081664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[191]: n_dims = 2, name = v.blk.11.attn_out.weight, tensor_size=2654208, offset=399086272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[192]: n_dims = 1, name = v.blk.11.attn_out.bias, tensor_size=4608, offset=401740480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[193]: n_dims = 1, name = v.blk.11.ln1.weight, tensor_size=4608, offset=401745088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[194]: n_dims = 1, name = v.blk.11.ln1.bias, tensor_size=4608, offset=401749696, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[195]: n_dims = 2, name = v.blk.11.ffn_down.weight, tensor_size=9916416, offset=401754304, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[196]: n_dims = 1, name = v.blk.11.ffn_down.bias, tensor_size=17216, offset=411670720, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[197]: n_dims = 2, name = v.blk.11.ffn_up.weight, tensor_size=9916416, offset=411687936, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[198]: n_dims = 1, name = v.blk.11.ffn_up.bias, tensor_size=4608, offset=421604352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[199]: n_dims = 1, name = v.blk.11.ln2.weight, tensor_size=4608, offset=421608960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[200]: n_dims = 1, name = v.blk.11.ln2.bias, tensor_size=4608, offset=421613568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[201]: n_dims = 2, name = v.blk.12.attn_q.weight, tensor_size=2654208, offset=421618176, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[202]: n_dims = 1, name = v.blk.12.attn_q.bias, tensor_size=4608, offset=424272384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[203]: n_dims = 2, name = v.blk.12.attn_k.weight, tensor_size=2654208, offset=424276992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[204]: n_dims = 1, name = v.blk.12.attn_k.bias, tensor_size=4608, offset=426931200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[205]: n_dims = 2, name = v.blk.12.attn_v.weight, tensor_size=2654208, offset=426935808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[206]: n_dims = 1, name = v.blk.12.attn_v.bias, tensor_size=4608, offset=429590016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[207]: n_dims = 2, name = v.blk.12.attn_out.weight, tensor_size=2654208, offset=429594624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[208]: n_dims = 1, name = v.blk.12.attn_out.bias, tensor_size=4608, offset=432248832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[209]: n_dims = 1, name = v.blk.12.ln1.weight, tensor_size=4608, offset=432253440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[210]: n_dims = 1, name = v.blk.12.ln1.bias, tensor_size=4608, offset=432258048, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[211]: n_dims = 2, name = v.blk.12.ffn_down.weight, tensor_size=9916416, offset=432262656, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[212]: n_dims = 1, name = v.blk.12.ffn_down.bias, tensor_size=17216, offset=442179072, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[213]: n_dims = 2, name = v.blk.12.ffn_up.weight, tensor_size=9916416, offset=442196288, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[214]: n_dims = 1, name = v.blk.12.ffn_up.bias, tensor_size=4608, offset=452112704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[215]: n_dims = 1, name = v.blk.12.ln2.weight, tensor_size=4608, offset=452117312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[216]: n_dims = 1, name = v.blk.12.ln2.bias, tensor_size=4608, offset=452121920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[217]: n_dims = 2, name = v.blk.13.attn_q.weight, tensor_size=2654208, offset=452126528, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[218]: n_dims = 1, name = v.blk.13.attn_q.bias, tensor_size=4608, offset=454780736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[219]: n_dims = 2, name = v.blk.13.attn_k.weight, tensor_size=2654208, offset=454785344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[220]: n_dims = 1, name = v.blk.13.attn_k.bias, tensor_size=4608, offset=457439552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[221]: n_dims = 2, name = v.blk.13.attn_v.weight, tensor_size=2654208, offset=457444160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[222]: n_dims = 1, name = v.blk.13.attn_v.bias, tensor_size=4608, offset=460098368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[223]: n_dims = 2, name = v.blk.13.attn_out.weight, tensor_size=2654208, offset=460102976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[224]: n_dims = 1, name = v.blk.13.attn_out.bias, tensor_size=4608, offset=462757184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[225]: n_dims = 1, name = v.blk.13.ln1.weight, tensor_size=4608, offset=462761792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[226]: n_dims = 1, name = v.blk.13.ln1.bias, tensor_size=4608, offset=462766400, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[227]: n_dims = 2, name = v.blk.13.ffn_down.weight, tensor_size=9916416, offset=462771008, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[228]: n_dims = 1, name = v.blk.13.ffn_down.bias, tensor_size=17216, offset=472687424, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[229]: n_dims = 2, name = v.blk.13.ffn_up.weight, tensor_size=9916416, offset=472704640, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[230]: n_dims = 1, name = v.blk.13.ffn_up.bias, tensor_size=4608, offset=482621056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[231]: n_dims = 1, name = v.blk.13.ln2.weight, tensor_size=4608, offset=482625664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[232]: n_dims = 1, name = v.blk.13.ln2.bias, tensor_size=4608, offset=482630272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[233]: n_dims = 2, name = v.blk.14.attn_q.weight, tensor_size=2654208, offset=482634880, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[234]: n_dims = 1, name = v.blk.14.attn_q.bias, tensor_size=4608, offset=485289088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[235]: n_dims = 2, name = v.blk.14.attn_k.weight, tensor_size=2654208, offset=485293696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[236]: n_dims = 1, name = v.blk.14.attn_k.bias, tensor_size=4608, offset=487947904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[237]: n_dims = 2, name = v.blk.14.attn_v.weight, tensor_size=2654208, offset=487952512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[238]: n_dims = 1, name = v.blk.14.attn_v.bias, tensor_size=4608, offset=490606720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[239]: n_dims = 2, name = v.blk.14.attn_out.weight, tensor_size=2654208, offset=490611328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[240]: n_dims = 1, name = v.blk.14.attn_out.bias, tensor_size=4608, offset=493265536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[241]: n_dims = 1, name = v.blk.14.ln1.weight, tensor_size=4608, offset=493270144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[242]: n_dims = 1, name = v.blk.14.ln1.bias, tensor_size=4608, offset=493274752, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[243]: n_dims = 2, name = v.blk.14.ffn_down.weight, tensor_size=9916416, offset=493279360, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[244]: n_dims = 1, name = v.blk.14.ffn_down.bias, tensor_size=17216, offset=503195776, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[245]: n_dims = 2, name = v.blk.14.ffn_up.weight, tensor_size=9916416, offset=503212992, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[246]: n_dims = 1, name = v.blk.14.ffn_up.bias, tensor_size=4608, offset=513129408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[247]: n_dims = 1, name = v.blk.14.ln2.weight, tensor_size=4608, offset=513134016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[248]: n_dims = 1, name = v.blk.14.ln2.bias, tensor_size=4608, offset=513138624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[249]: n_dims = 2, name = v.blk.15.attn_q.weight, tensor_size=2654208, offset=513143232, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[250]: n_dims = 1, name = v.blk.15.attn_q.bias, tensor_size=4608, offset=515797440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[251]: n_dims = 2, name = v.blk.15.attn_k.weight, tensor_size=2654208, offset=515802048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[252]: n_dims = 1, name = v.blk.15.attn_k.bias, tensor_size=4608, offset=518456256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[253]: n_dims = 2, name = v.blk.15.attn_v.weight, tensor_size=2654208, offset=518460864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[254]: n_dims = 1, name = v.blk.15.attn_v.bias, tensor_size=4608, offset=521115072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[255]: n_dims = 2, name = v.blk.15.attn_out.weight, tensor_size=2654208, offset=521119680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[256]: n_dims = 1, name = v.blk.15.attn_out.bias, tensor_size=4608, offset=523773888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[257]: n_dims = 1, name = v.blk.15.ln1.weight, tensor_size=4608, offset=523778496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[258]: n_dims = 1, name = v.blk.15.ln1.bias, tensor_size=4608, offset=523783104, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[259]: n_dims = 2, name = v.blk.15.ffn_down.weight, tensor_size=9916416, offset=523787712, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[260]: n_dims = 1, name = v.blk.15.ffn_down.bias, tensor_size=17216, offset=533704128, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[261]: n_dims = 2, name = v.blk.15.ffn_up.weight, tensor_size=9916416, offset=533721344, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[262]: n_dims = 1, name = v.blk.15.ffn_up.bias, tensor_size=4608, offset=543637760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[263]: n_dims = 1, name = v.blk.15.ln2.weight, tensor_size=4608, offset=543642368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[264]: n_dims = 1, name = v.blk.15.ln2.bias, tensor_size=4608, offset=543646976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[265]: n_dims = 2, name = v.blk.16.attn_q.weight, tensor_size=2654208, offset=543651584, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[266]: n_dims = 1, name = v.blk.16.attn_q.bias, tensor_size=4608, offset=546305792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[267]: n_dims = 2, name = v.blk.16.attn_k.weight, tensor_size=2654208, offset=546310400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[268]: n_dims = 1, name = v.blk.16.attn_k.bias, tensor_size=4608, offset=548964608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[269]: n_dims = 2, name = v.blk.16.attn_v.weight, tensor_size=2654208, offset=548969216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[270]: n_dims = 1, name = v.blk.16.attn_v.bias, tensor_size=4608, offset=551623424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[271]: n_dims = 2, name = v.blk.16.attn_out.weight, tensor_size=2654208, offset=551628032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[272]: n_dims = 1, name = v.blk.16.attn_out.bias, tensor_size=4608, offset=554282240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[273]: n_dims = 1, name = v.blk.16.ln1.weight, tensor_size=4608, offset=554286848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[274]: n_dims = 1, name = v.blk.16.ln1.bias, tensor_size=4608, offset=554291456, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[275]: n_dims = 2, name = v.blk.16.ffn_down.weight, tensor_size=9916416, offset=554296064, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[276]: n_dims = 1, name = v.blk.16.ffn_down.bias, tensor_size=17216, offset=564212480, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[277]: n_dims = 2, name = v.blk.16.ffn_up.weight, tensor_size=9916416, offset=564229696, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[278]: n_dims = 1, name = v.blk.16.ffn_up.bias, tensor_size=4608, offset=574146112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[279]: n_dims = 1, name = v.blk.16.ln2.weight, tensor_size=4608, offset=574150720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[280]: n_dims = 1, name = v.blk.16.ln2.bias, tensor_size=4608, offset=574155328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[281]: n_dims = 2, name = v.blk.17.attn_q.weight, tensor_size=2654208, offset=574159936, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[282]: n_dims = 1, name = v.blk.17.attn_q.bias, tensor_size=4608, offset=576814144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[283]: n_dims = 2, name = v.blk.17.attn_k.weight, tensor_size=2654208, offset=576818752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[284]: n_dims = 1, name = v.blk.17.attn_k.bias, tensor_size=4608, offset=579472960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[285]: n_dims = 2, name = v.blk.17.attn_v.weight, tensor_size=2654208, offset=579477568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[286]: n_dims = 1, name = v.blk.17.attn_v.bias, tensor_size=4608, offset=582131776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[287]: n_dims = 2, name = v.blk.17.attn_out.weight, tensor_size=2654208, offset=582136384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[288]: n_dims = 1, name = v.blk.17.attn_out.bias, tensor_size=4608, offset=584790592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[289]: n_dims = 1, name = v.blk.17.ln1.weight, tensor_size=4608, offset=584795200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[290]: n_dims = 1, name = v.blk.17.ln1.bias, tensor_size=4608, offset=584799808, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[291]: n_dims = 2, name = v.blk.17.ffn_down.weight, tensor_size=9916416, offset=584804416, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[292]: n_dims = 1, name = v.blk.17.ffn_down.bias, tensor_size=17216, offset=594720832, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[293]: n_dims = 2, name = v.blk.17.ffn_up.weight, tensor_size=9916416, offset=594738048, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[294]: n_dims = 1, name = v.blk.17.ffn_up.bias, tensor_size=4608, offset=604654464, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[295]: n_dims = 1, name = v.blk.17.ln2.weight, tensor_size=4608, offset=604659072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[296]: n_dims = 1, name = v.blk.17.ln2.bias, tensor_size=4608, offset=604663680, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[297]: n_dims = 2, name = v.blk.18.attn_q.weight, tensor_size=2654208, offset=604668288, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[298]: n_dims = 1, name = v.blk.18.attn_q.bias, tensor_size=4608, offset=607322496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[299]: n_dims = 2, name = v.blk.18.attn_k.weight, tensor_size=2654208, offset=607327104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[300]: n_dims = 1, name = v.blk.18.attn_k.bias, tensor_size=4608, offset=609981312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[301]: n_dims = 2, name = v.blk.18.attn_v.weight, tensor_size=2654208, offset=609985920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[302]: n_dims = 1, name = v.blk.18.attn_v.bias, tensor_size=4608, offset=612640128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[303]: n_dims = 2, name = v.blk.18.attn_out.weight, tensor_size=2654208, offset=612644736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[304]: n_dims = 1, name = v.blk.18.attn_out.bias, tensor_size=4608, offset=615298944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[305]: n_dims = 1, name = v.blk.18.ln1.weight, tensor_size=4608, offset=615303552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[306]: n_dims = 1, name = v.blk.18.ln1.bias, tensor_size=4608, offset=615308160, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[307]: n_dims = 2, name = v.blk.18.ffn_down.weight, tensor_size=9916416, offset=615312768, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[308]: n_dims = 1, name = v.blk.18.ffn_down.bias, tensor_size=17216, offset=625229184, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[309]: n_dims = 2, name = v.blk.18.ffn_up.weight, tensor_size=9916416, offset=625246400, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[310]: n_dims = 1, name = v.blk.18.ffn_up.bias, tensor_size=4608, offset=635162816, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[311]: n_dims = 1, name = v.blk.18.ln2.weight, tensor_size=4608, offset=635167424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[312]: n_dims = 1, name = v.blk.18.ln2.bias, tensor_size=4608, offset=635172032, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[313]: n_dims = 2, name = v.blk.19.attn_q.weight, tensor_size=2654208, offset=635176640, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[314]: n_dims = 1, name = v.blk.19.attn_q.bias, tensor_size=4608, offset=637830848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[315]: n_dims = 2, name = v.blk.19.attn_k.weight, tensor_size=2654208, offset=637835456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[316]: n_dims = 1, name = v.blk.19.attn_k.bias, tensor_size=4608, offset=640489664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[317]: n_dims = 2, name = v.blk.19.attn_v.weight, tensor_size=2654208, offset=640494272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[318]: n_dims = 1, name = v.blk.19.attn_v.bias, tensor_size=4608, offset=643148480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[319]: n_dims = 2, name = v.blk.19.attn_out.weight, tensor_size=2654208, offset=643153088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[320]: n_dims = 1, name = v.blk.19.attn_out.bias, tensor_size=4608, offset=645807296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[321]: n_dims = 1, name = v.blk.19.ln1.weight, tensor_size=4608, offset=645811904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[322]: n_dims = 1, name = v.blk.19.ln1.bias, tensor_size=4608, offset=645816512, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[323]: n_dims = 2, name = v.blk.19.ffn_down.weight, tensor_size=9916416, offset=645821120, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[324]: n_dims = 1, name = v.blk.19.ffn_down.bias, tensor_size=17216, offset=655737536, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[325]: n_dims = 2, name = v.blk.19.ffn_up.weight, tensor_size=9916416, offset=655754752, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[326]: n_dims = 1, name = v.blk.19.ffn_up.bias, tensor_size=4608, offset=665671168, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[327]: n_dims = 1, name = v.blk.19.ln2.weight, tensor_size=4608, offset=665675776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[328]: n_dims = 1, name = v.blk.19.ln2.bias, tensor_size=4608, offset=665680384, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[329]: n_dims = 2, name = v.blk.20.attn_q.weight, tensor_size=2654208, offset=665684992, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[330]: n_dims = 1, name = v.blk.20.attn_q.bias, tensor_size=4608, offset=668339200, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[331]: n_dims = 2, name = v.blk.20.attn_k.weight, tensor_size=2654208, offset=668343808, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[332]: n_dims = 1, name = v.blk.20.attn_k.bias, tensor_size=4608, offset=670998016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[333]: n_dims = 2, name = v.blk.20.attn_v.weight, tensor_size=2654208, offset=671002624, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[334]: n_dims = 1, name = v.blk.20.attn_v.bias, tensor_size=4608, offset=673656832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[335]: n_dims = 2, name = v.blk.20.attn_out.weight, tensor_size=2654208, offset=673661440, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[336]: n_dims = 1, name = v.blk.20.attn_out.bias, tensor_size=4608, offset=676315648, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[337]: n_dims = 1, name = v.blk.20.ln1.weight, tensor_size=4608, offset=676320256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[338]: n_dims = 1, name = v.blk.20.ln1.bias, tensor_size=4608, offset=676324864, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[339]: n_dims = 2, name = v.blk.20.ffn_down.weight, tensor_size=9916416, offset=676329472, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[340]: n_dims = 1, name = v.blk.20.ffn_down.bias, tensor_size=17216, offset=686245888, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[341]: n_dims = 2, name = v.blk.20.ffn_up.weight, tensor_size=9916416, offset=686263104, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[342]: n_dims = 1, name = v.blk.20.ffn_up.bias, tensor_size=4608, offset=696179520, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[343]: n_dims = 1, name = v.blk.20.ln2.weight, tensor_size=4608, offset=696184128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[344]: n_dims = 1, name = v.blk.20.ln2.bias, tensor_size=4608, offset=696188736, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[345]: n_dims = 2, name = v.blk.21.attn_q.weight, tensor_size=2654208, offset=696193344, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[346]: n_dims = 1, name = v.blk.21.attn_q.bias, tensor_size=4608, offset=698847552, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[347]: n_dims = 2, name = v.blk.21.attn_k.weight, tensor_size=2654208, offset=698852160, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[348]: n_dims = 1, name = v.blk.21.attn_k.bias, tensor_size=4608, offset=701506368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[349]: n_dims = 2, name = v.blk.21.attn_v.weight, tensor_size=2654208, offset=701510976, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[350]: n_dims = 1, name = v.blk.21.attn_v.bias, tensor_size=4608, offset=704165184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[351]: n_dims = 2, name = v.blk.21.attn_out.weight, tensor_size=2654208, offset=704169792, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[352]: n_dims = 1, name = v.blk.21.attn_out.bias, tensor_size=4608, offset=706824000, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[353]: n_dims = 1, name = v.blk.21.ln1.weight, tensor_size=4608, offset=706828608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[354]: n_dims = 1, name = v.blk.21.ln1.bias, tensor_size=4608, offset=706833216, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[355]: n_dims = 2, name = v.blk.21.ffn_down.weight, tensor_size=9916416, offset=706837824, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[356]: n_dims = 1, name = v.blk.21.ffn_down.bias, tensor_size=17216, offset=716754240, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[357]: n_dims = 2, name = v.blk.21.ffn_up.weight, tensor_size=9916416, offset=716771456, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[358]: n_dims = 1, name = v.blk.21.ffn_up.bias, tensor_size=4608, offset=726687872, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[359]: n_dims = 1, name = v.blk.21.ln2.weight, tensor_size=4608, offset=726692480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[360]: n_dims = 1, name = v.blk.21.ln2.bias, tensor_size=4608, offset=726697088, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[361]: n_dims = 2, name = v.blk.22.attn_q.weight, tensor_size=2654208, offset=726701696, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[362]: n_dims = 1, name = v.blk.22.attn_q.bias, tensor_size=4608, offset=729355904, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[363]: n_dims = 2, name = v.blk.22.attn_k.weight, tensor_size=2654208, offset=729360512, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[364]: n_dims = 1, name = v.blk.22.attn_k.bias, tensor_size=4608, offset=732014720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[365]: n_dims = 2, name = v.blk.22.attn_v.weight, tensor_size=2654208, offset=732019328, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[366]: n_dims = 1, name = v.blk.22.attn_v.bias, tensor_size=4608, offset=734673536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[367]: n_dims = 2, name = v.blk.22.attn_out.weight, tensor_size=2654208, offset=734678144, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[368]: n_dims = 1, name = v.blk.22.attn_out.bias, tensor_size=4608, offset=737332352, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[369]: n_dims = 1, name = v.blk.22.ln1.weight, tensor_size=4608, offset=737336960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[370]: n_dims = 1, name = v.blk.22.ln1.bias, tensor_size=4608, offset=737341568, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[371]: n_dims = 2, name = v.blk.22.ffn_down.weight, tensor_size=9916416, offset=737346176, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[372]: n_dims = 1, name = v.blk.22.ffn_down.bias, tensor_size=17216, offset=747262592, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[373]: n_dims = 2, name = v.blk.22.ffn_up.weight, tensor_size=9916416, offset=747279808, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[374]: n_dims = 1, name = v.blk.22.ffn_up.bias, tensor_size=4608, offset=757196224, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[375]: n_dims = 1, name = v.blk.22.ln2.weight, tensor_size=4608, offset=757200832, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[376]: n_dims = 1, name = v.blk.22.ln2.bias, tensor_size=4608, offset=757205440, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[377]: n_dims = 2, name = v.blk.23.attn_q.weight, tensor_size=2654208, offset=757210048, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[378]: n_dims = 1, name = v.blk.23.attn_q.bias, tensor_size=4608, offset=759864256, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[379]: n_dims = 2, name = v.blk.23.attn_k.weight, tensor_size=2654208, offset=759868864, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[380]: n_dims = 1, name = v.blk.23.attn_k.bias, tensor_size=4608, offset=762523072, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[381]: n_dims = 2, name = v.blk.23.attn_v.weight, tensor_size=2654208, offset=762527680, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[382]: n_dims = 1, name = v.blk.23.attn_v.bias, tensor_size=4608, offset=765181888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[383]: n_dims = 2, name = v.blk.23.attn_out.weight, tensor_size=2654208, offset=765186496, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[384]: n_dims = 1, name = v.blk.23.attn_out.bias, tensor_size=4608, offset=767840704, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[385]: n_dims = 1, name = v.blk.23.ln1.weight, tensor_size=4608, offset=767845312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[386]: n_dims = 1, name = v.blk.23.ln1.bias, tensor_size=4608, offset=767849920, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[387]: n_dims = 2, name = v.blk.23.ffn_down.weight, tensor_size=9916416, offset=767854528, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[388]: n_dims = 1, name = v.blk.23.ffn_down.bias, tensor_size=17216, offset=777770944, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[389]: n_dims = 2, name = v.blk.23.ffn_up.weight, tensor_size=9916416, offset=777788160, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[390]: n_dims = 1, name = v.blk.23.ffn_up.bias, tensor_size=4608, offset=787704576, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[391]: n_dims = 1, name = v.blk.23.ln2.weight, tensor_size=4608, offset=787709184, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[392]: n_dims = 1, name = v.blk.23.ln2.bias, tensor_size=4608, offset=787713792, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[393]: n_dims = 2, name = v.blk.24.attn_q.weight, tensor_size=2654208, offset=787718400, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[394]: n_dims = 1, name = v.blk.24.attn_q.bias, tensor_size=4608, offset=790372608, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[395]: n_dims = 2, name = v.blk.24.attn_k.weight, tensor_size=2654208, offset=790377216, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[396]: n_dims = 1, name = v.blk.24.attn_k.bias, tensor_size=4608, offset=793031424, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[397]: n_dims = 2, name = v.blk.24.attn_v.weight, tensor_size=2654208, offset=793036032, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[398]: n_dims = 1, name = v.blk.24.attn_v.bias, tensor_size=4608, offset=795690240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[399]: n_dims = 2, name = v.blk.24.attn_out.weight, tensor_size=2654208, offset=795694848, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[400]: n_dims = 1, name = v.blk.24.attn_out.bias, tensor_size=4608, offset=798349056, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[401]: n_dims = 1, name = v.blk.24.ln1.weight, tensor_size=4608, offset=798353664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[402]: n_dims = 1, name = v.blk.24.ln1.bias, tensor_size=4608, offset=798358272, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[403]: n_dims = 2, name = v.blk.24.ffn_down.weight, tensor_size=9916416, offset=798362880, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[404]: n_dims = 1, name = v.blk.24.ffn_down.bias, tensor_size=17216, offset=808279296, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[405]: n_dims = 2, name = v.blk.24.ffn_up.weight, tensor_size=9916416, offset=808296512, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[406]: n_dims = 1, name = v.blk.24.ffn_up.bias, tensor_size=4608, offset=818212928, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[407]: n_dims = 1, name = v.blk.24.ln2.weight, tensor_size=4608, offset=818217536, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[408]: n_dims = 1, name = v.blk.24.ln2.bias, tensor_size=4608, offset=818222144, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[409]: n_dims = 2, name = v.blk.25.attn_q.weight, tensor_size=2654208, offset=818226752, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[410]: n_dims = 1, name = v.blk.25.attn_q.bias, tensor_size=4608, offset=820880960, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[411]: n_dims = 2, name = v.blk.25.attn_k.weight, tensor_size=2654208, offset=820885568, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[412]: n_dims = 1, name = v.blk.25.attn_k.bias, tensor_size=4608, offset=823539776, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[413]: n_dims = 2, name = v.blk.25.attn_v.weight, tensor_size=2654208, offset=823544384, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[414]: n_dims = 1, name = v.blk.25.attn_v.bias, tensor_size=4608, offset=826198592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[415]: n_dims = 2, name = v.blk.25.attn_out.weight, tensor_size=2654208, offset=826203200, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[416]: n_dims = 1, name = v.blk.25.attn_out.bias, tensor_size=4608, offset=828857408, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[417]: n_dims = 1, name = v.blk.25.ln1.weight, tensor_size=4608, offset=828862016, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[418]: n_dims = 1, name = v.blk.25.ln1.bias, tensor_size=4608, offset=828866624, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[419]: n_dims = 2, name = v.blk.25.ffn_down.weight, tensor_size=9916416, offset=828871232, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[420]: n_dims = 1, name = v.blk.25.ffn_down.bias, tensor_size=17216, offset=838787648, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[421]: n_dims = 2, name = v.blk.25.ffn_up.weight, tensor_size=9916416, offset=838804864, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[422]: n_dims = 1, name = v.blk.25.ffn_up.bias, tensor_size=4608, offset=848721280, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[423]: n_dims = 1, name = v.blk.25.ln2.weight, tensor_size=4608, offset=848725888, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[424]: n_dims = 1, name = v.blk.25.ln2.bias, tensor_size=4608, offset=848730496, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[425]: n_dims = 2, name = v.blk.26.attn_q.weight, tensor_size=2654208, offset=848735104, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[426]: n_dims = 1, name = v.blk.26.attn_q.bias, tensor_size=4608, offset=851389312, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[427]: n_dims = 2, name = v.blk.26.attn_k.weight, tensor_size=2654208, offset=851393920, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[428]: n_dims = 1, name = v.blk.26.attn_k.bias, tensor_size=4608, offset=854048128, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[429]: n_dims = 2, name = v.blk.26.attn_v.weight, tensor_size=2654208, offset=854052736, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[430]: n_dims = 1, name = v.blk.26.attn_v.bias, tensor_size=4608, offset=856706944, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[431]: n_dims = 2, name = v.blk.26.attn_out.weight, tensor_size=2654208, offset=856711552, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[432]: n_dims = 1, name = v.blk.26.attn_out.bias, tensor_size=4608, offset=859365760, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[433]: n_dims = 1, name = v.blk.26.ln1.weight, tensor_size=4608, offset=859370368, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[434]: n_dims = 1, name = v.blk.26.ln1.bias, tensor_size=4608, offset=859374976, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[435]: n_dims = 2, name = v.blk.26.ffn_down.weight, tensor_size=9916416, offset=859379584, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[436]: n_dims = 1, name = v.blk.26.ffn_down.bias, tensor_size=17216, offset=869296000, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[437]: n_dims = 2, name = v.blk.26.ffn_up.weight, tensor_size=9916416, offset=869313216, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[438]: n_dims = 1, name = v.blk.26.ffn_up.bias, tensor_size=4608, offset=879229632, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[439]: n_dims = 1, name = v.blk.26.ln2.weight, tensor_size=4608, offset=879234240, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[440]: n_dims = 1, name = v.blk.26.ln2.bias, tensor_size=4608, offset=879238848, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[441]: n_dims = 2, name = v.blk.27.attn_q.weight, tensor_size=2654208, offset=879243456, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[442]: n_dims = 1, name = v.blk.27.attn_q.bias, tensor_size=4608, offset=881897664, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[443]: n_dims = 2, name = v.blk.27.attn_k.weight, tensor_size=2654208, offset=881902272, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[444]: n_dims = 1, name = v.blk.27.attn_k.bias, tensor_size=4608, offset=884556480, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[445]: n_dims = 2, name = v.blk.27.attn_v.weight, tensor_size=2654208, offset=884561088, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[446]: n_dims = 1, name = v.blk.27.attn_v.bias, tensor_size=4608, offset=887215296, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[447]: n_dims = 2, name = v.blk.27.attn_out.weight, tensor_size=2654208, offset=887219904, shape:[1152, 1152, 1, 1], type = f16
clip_model_loader: tensor[448]: n_dims = 1, name = v.blk.27.attn_out.bias, tensor_size=4608, offset=889874112, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[449]: n_dims = 1, name = v.blk.27.ln1.weight, tensor_size=4608, offset=889878720, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[450]: n_dims = 1, name = v.blk.27.ln1.bias, tensor_size=4608, offset=889883328, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[451]: n_dims = 2, name = v.blk.27.ffn_down.weight, tensor_size=9916416, offset=889887936, shape:[1152, 4304, 1, 1], type = f16
clip_model_loader: tensor[452]: n_dims = 1, name = v.blk.27.ffn_down.bias, tensor_size=17216, offset=899804352, shape:[4304, 1, 1, 1], type = f32
clip_model_loader: tensor[453]: n_dims = 2, name = v.blk.27.ffn_up.weight, tensor_size=9916416, offset=899821568, shape:[4304, 1152, 1, 1], type = f16
clip_model_loader: tensor[454]: n_dims = 1, name = v.blk.27.ffn_up.bias, tensor_size=4608, offset=909737984, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[455]: n_dims = 1, name = v.blk.27.ln2.weight, tensor_size=4608, offset=909742592, shape:[1152, 1, 1, 1], type = f32
clip_model_loader: tensor[456]: n_dims = 1, name = v.blk.27.ln2.bias, tensor_size=4608, offset=909747200, shape:[1152, 1, 1, 1], type = f32
clip_ctx: CLIP using CUDA0 backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1152
load_hparams: n_head:             16
load_hparams: n_ff:               4304
load_hparams: n_layer:            28
load_hparams: ffn_op:             gelu
load_hparams: projection_dim:     2048

--- vision hparams ---
load_hparams: image_size:         378
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: n_merge:            0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         867.61 MiB
load_hparams: metadata size:      0.16 MiB
load_tensors: ffn up/down are swapped
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
NvMapMemAllocInternalTagged: 1075072515 error 12
NvMapMemHandleAlloc: error 0
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 867.61 MiB on device 0: cudaMalloc failed: out of memory
alloc_tensor_range: failed to allocate CUDA0 buffer of size 909753600
ggml-backend.cpp:199: GGML_ASSERT(buffer) failed
[New LWP 10625]
[New LWP 10626]
[New LWP 10627]
[New LWP 10628]
[New LWP 10629]
[New LWP 10630]
[New LWP 10631]
[New LWP 10634]
time=2026-02-15T12:55:10.593-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server not responding"
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/aarch64-linux-gnu/libthread_db.so.1".
0x0000aaaad4bc116c in ?? ()
#0  0x0000aaaad4bc116c in ?? ()
#1  0x0000000000000080 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
[Inferior 1 (process 10624) detached]
SIGABRT: abort
PC=0xffff954a2008 m=4 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 15 gp=0x4000103500 m=4 mp=0x4000065808 [syscall]:
runtime.cgocall(0xaaaad5825fbc, 0x40000edc08)
	runtime/cgocall.go:167 +0x44 fp=0x40000edbd0 sp=0x40000edb90 pc=0xaaaad4bb4914
github.com/ollama/ollama/llama._Cfunc_mtmd_init_from_file(0xffff38000bf0, 0xffff38000c70, {0x1, 0x1, 0x4, 0xaaaad5d56eef, 0xaaaad5d56ee3, 0xffffffff, 0x1, 0xffffffff, ...})
	_cgo_gotypes.go:1134 +0x34 fp=0x40000edc00 sp=0x40000edbd0 pc=0xaaaad4f6e4b4
github.com/ollama/ollama/llama.NewMtmdContext.func2(...)
	github.com/ollama/ollama/llama/llama.go:539
github.com/ollama/ollama/llama.NewMtmdContext(0x4000328300, {0x4000096380, 0x70})
	github.com/ollama/ollama/llama/llama.go:539 +0x12c fp=0x40000edd20 sp=0x40000edc00 pc=0xaaaad4f71c0c
github.com/ollama/ollama/runner/llamarunner.NewImageContext(0x4000328300, {0x4000096380, 0x70})
	github.com/ollama/ollama/runner/llamarunner/image.go:35 +0xe8 fp=0x40000edda0 sp=0x40000edd20 pc=0xaaaad5011328
github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0x40004ca8c0, {{0x4000511c40, 0x1, 0x1}, 0x19, 0x0, 0x1, {0x4000511c38, 0x1, 0x2}, ...}, ...)
	github.com/ollama/ollama/runner/llamarunner/runner.go:861 +0x1d8 fp=0x40000edef0 sp=0x40000edda0 pc=0xaaaad5015998
github.com/ollama/ollama/runner/llamarunner.(*Server).load.gowrap2()
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0xb0 fp=0x40000edfd0 sp=0x40000edef0 pc=0xaaaad5016730
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000edfd0 sp=0x40000edfd0 pc=0xaaaad4bbfd14
created by github.com/ollama/ollama/runner/llamarunner.(*Server).load in goroutine 12
	github.com/ollama/ollama/runner/llamarunner/runner.go:934 +0x6fc

goroutine 1 gp=0x40000021c0 m=nil [runnable]:
syscall.Syscall6(0xd0, 0xa, 0x6, 0x6, 0x40005977cc, 0x4, 0x0)
	syscall/syscall_linux.go:95 +0x2c fp=0x4000597740 sp=0x40005976e0 pc=0xaaaad4bdc60c
syscall.setsockopt(0xffff953e0f30?, 0x0?, 0x6?, 0x0?, 0x4000597808?)
	syscall/zsyscall_linux_arm64.go:1413 +0x3c fp=0x4000597790 sp=0x4000597740 pc=0xaaaad4bdc79c
syscall.SetsockoptInt(...)
	syscall/syscall_unix.go:464
internal/poll.(*FD).SetsockoptInt(0x4000597878?, 0xaaaad4cc2530?, 0xaaaad4b62650?, 0x4000597868?)
	internal/poll/sockopt.go:17 +0x108 fp=0x4000597800 sp=0x4000597790 pc=0xaaaad4c40058
net.setKeepAliveCount(0x400038c000, 0x4000062870?)
	net/tcpsockopt_unix.go:50 +0x40 fp=0x4000597840 sp=0x4000597800 pc=0xaaaad4cc2880
net.(*TCPConn).SetKeepAliveConfig(0x4000062870, {0x4c?, 0xa?, 0x0?, 0x4000036020?})
	net/tcpsock_unix.go:26 +0x2fc fp=0x40005978c0 sp=0x4000597840 pc=0xaaaad4cc23bc
net.newTCPConn(0x400038c000, 0x0, {0x30?, 0xffff953ebdd0?, 0xffff953e13e8?, 0x20?}, 0xaaaad6094f78, 0xaaaad6094f80)
	net/tcpsock.go:302 +0xf8 fp=0x4000597920 sp=0x40005978c0 pc=0xaaaad4cc0b98
net.(*TCPListener).accept(0x400041a640)
	net/tcpsock_posix.go:163 +0x64 fp=0x4000597970 sp=0x4000597920 pc=0xaaaad4cc1eb4
net.(*TCPListener).Accept(0x400041a640)
	net/tcpsock.go:380 +0x2c fp=0x40005979b0 sp=0x4000597970 pc=0xaaaad4cc0e0c
net/http.(*onceCloseListener).Accept(0x400022e480?)
	<autogenerated>:1 +0x30 fp=0x40005979d0 sp=0x40005979b0 pc=0xaaaad4e9b360
net/http.(*Server).Serve(0x40001f3900, {0xaaaad60a9da0, 0x400041a640})
	net/http/server.go:3424 +0x290 fp=0x4000597b00 sp=0x40005979d0 pc=0xaaaad4e74aa0
github.com/ollama/ollama/runner/llamarunner.Execute({0x4000032260, 0x4, 0x4})
	github.com/ollama/ollama/runner/llamarunner/runner.go:1002 +0x7ac fp=0x4000597cd0 sp=0x4000597b00 pc=0xaaaad5016fcc
github.com/ollama/ollama/runner.Execute({0x4000032250?, 0x0?, 0x0?})
	github.com/ollama/ollama/runner/runner.go:22 +0x16c fp=0x4000597d10 sp=0x4000597cd0 pc=0xaaaad5109a1c
github.com/ollama/ollama/cmd.NewCLI.func3(0x40001f3700?, {0xaaaad5b210fd?, 0x4?, 0xaaaad5b21101?})
	github.com/ollama/ollama/cmd/cmd.go:1979 +0x54 fp=0x4000597d40 sp=0x4000597d10 pc=0xaaaad57c6924
github.com/spf13/cobra.(*Command).execute(0x4000487b08, {0x400041a400, 0x4, 0x4})
	github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x4000597e60 sp=0x4000597d40 pc=0xaaaad4d1c6d8
github.com/spf13/cobra.(*Command).ExecuteC(0x400013c908)
	github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x4000597f20 sp=0x4000597e60 pc=0xaaaad4d1ce20
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.7.0/command.go:992
github.com/spf13/cobra.(*Command).ExecuteContext(...)
	github.com/spf13/cobra@v1.7.0/command.go:985
main.main()
	github.com/ollama/ollama/main.go:12 +0x54 fp=0x4000597f40 sp=0x4000597f20 pc=0xaaaad57c7474
runtime.main()
	runtime/proc.go:283 +0x284 fp=0x4000597fd0 sp=0x4000597f40 pc=0xaaaad4b84194
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000597fd0 sp=0x4000597fd0 pc=0xaaaad4bbfd14

goroutine 2 gp=0x4000002c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ef90 sp=0x400005ef70 pc=0xaaaad4bb7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.forcegchelper()
	runtime/proc.go:348 +0xb8 fp=0x400005efd0 sp=0x400005ef90 pc=0xaaaad4b844e8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005efd0 sp=0x400005efd0 pc=0xaaaad4bbfd14
created by runtime.init.7 in goroutine 1
	runtime/proc.go:336 +0x24

goroutine 3 gp=0x4000003180 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005f760 sp=0x400005f740 pc=0xaaaad4bb7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.bgsweep(0x400008a000)
	runtime/mgcsweep.go:316 +0x108 fp=0x400005f7b0 sp=0x400005f760 pc=0xaaaad4b6ed18
runtime.gcenable.gowrap1()
	runtime/mgc.go:204 +0x28 fp=0x400005f7d0 sp=0x400005f7b0 pc=0xaaaad4b62b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005f7d0 sp=0x400005f7d0 pc=0xaaaad4bbfd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0x6c

goroutine 4 gp=0x4000003340 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0xaaaad69b2580?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005ff60 sp=0x400005ff40 pc=0xaaaad4bb7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.(*scavengerState).park(0xaaaad69b2580)
	runtime/mgcscavenge.go:425 +0x5c fp=0x400005ff90 sp=0x400005ff60 pc=0xaaaad4b6c7dc
runtime.bgscavenge(0x400008a000)
	runtime/mgcscavenge.go:658 +0xac fp=0x400005ffb0 sp=0x400005ff90 pc=0xaaaad4b6cd5c
runtime.gcenable.gowrap2()
	runtime/mgc.go:205 +0x28 fp=0x400005ffd0 sp=0x400005ffb0 pc=0xaaaad4b62ae8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005ffd0 sp=0x400005ffd0 pc=0xaaaad4bbfd14
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:205 +0xac

goroutine 5 gp=0x4000003c00 m=nil [finalizer wait]:
runtime.gopark(0x18000001b8?, 0x1000000000000?, 0xf8?, 0xe5?, 0xaaaad4e9ddac?)
	runtime/proc.go:435 +0xc8 fp=0x400005e590 sp=0x400005e570 pc=0xaaaad4bb7e28
runtime.runfinq()
	runtime/mfinal.go:196 +0x108 fp=0x400005e7d0 sp=0x400005e590 pc=0xaaaad4b61b48
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005e7d0 sp=0x400005e7d0 pc=0xaaaad4bbfd14
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:166 +0x80

goroutine 6 gp=0x40001de700 m=nil [chan receive]:
runtime.gopark(0x40001cb900?, 0x400050e018?, 0x48?, 0x7?, 0xaaaad4c84f98?)
	runtime/proc.go:435 +0xc8 fp=0x40000606f0 sp=0x40000606d0 pc=0xaaaad4bb7e28
runtime.chanrecv(0x4000096310, 0x0, 0x1)
	runtime/chan.go:664 +0x42c fp=0x4000060770 sp=0x40000606f0 pc=0xaaaad4b53bac
runtime.chanrecv1(0x0?, 0x0?)
	runtime/chan.go:506 +0x14 fp=0x40000607a0 sp=0x4000060770 pc=0xaaaad4b53744
runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
	runtime/mgc.go:1796
runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
	runtime/mgc.go:1799 +0x3c fp=0x40000607d0 sp=0x40000607a0 pc=0xaaaad4b65d6c
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000607d0 sp=0x40000607d0 pc=0xaaaad4bbfd14
created by unique.runtime_registerUniqueMapCleanup in goroutine 1
	runtime/mgc.go:1794 +0x78

goroutine 7 gp=0x40001dec40 m=nil [GC worker (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000060f10 sp=0x4000060ef0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x4000060fb0 sp=0x4000060f10 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000060fd0 sp=0x4000060fb0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000060fd0 sp=0x4000060fd0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 18 gp=0x4000102380 m=nil [GC worker (idle)]:
runtime.gopark(0x29852451503e?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400005a710 sp=0x400005a6f0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x400005a7b0 sp=0x400005a710 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400005a7d0 sp=0x400005a7b0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400005a7d0 sp=0x400005a7d0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 34 gp=0x4000504000 m=nil [GC worker (idle)]:
runtime.gopark(0x2985244fcfbb?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400050a710 sp=0x400050a6f0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x400050a7b0 sp=0x400050a710 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x400050a7d0 sp=0x400050a7b0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400050a7d0 sp=0x400050a7d0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 8 gp=0x40001dee00 m=nil [GC worker (idle)]:
runtime.gopark(0x2985244fd99b?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000061710 sp=0x40000616f0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x40000617b0 sp=0x4000061710 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40000617d0 sp=0x40000617b0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40000617d0 sp=0x40000617d0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 9 gp=0x40001defc0 m=nil [GC worker (idle)]:
runtime.gopark(0x2985244fcfbb?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000061f10 sp=0x4000061ef0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x4000061fb0 sp=0x4000061f10 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x4000061fd0 sp=0x4000061fb0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x4000061fd0 sp=0x4000061fd0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 10 gp=0x40001df180 m=nil [GC worker (idle)]:
runtime.gopark(0x2985244fd2db?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x4000506710 sp=0x40005066f0 pc=0xaaaad4bb7e28
runtime.gcBgMarkWorker(0x4000097ab0)
	runtime/mgc.go:1423 +0xdc fp=0x40005067b0 sp=0x4000506710 pc=0xaaaad4b64fdc
runtime.gcBgMarkStartWorkers.gowrap1()
	runtime/mgc.go:1339 +0x28 fp=0x40005067d0 sp=0x40005067b0 pc=0xaaaad4b64ec8
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x40005067d0 sp=0x40005067d0 pc=0xaaaad4bbfd14
created by runtime.gcBgMarkStartWorkers in goroutine 1
	runtime/mgc.go:1339 +0x140

goroutine 11 gp=0x4000102fc0 m=nil [sync.WaitGroup.Wait]:
runtime.gopark(0xaaaad69c6880?, 0x0?, 0x80?, 0xa1?, 0x0?)
	runtime/proc.go:435 +0xc8 fp=0x400050de20 sp=0x400050de00 pc=0xaaaad4bb7e28
runtime.goparkunlock(...)
	runtime/proc.go:441
runtime.semacquire1(0x40004ca8e0, 0x0, 0x1, 0x0, 0x18)
	runtime/sema.go:188 +0x204 fp=0x400050de70 sp=0x400050de20 pc=0xaaaad4b98634
sync.runtime_SemacquireWaitGroup(0x0?)
	runtime/sema.go:110 +0x2c fp=0x400050deb0 sp=0x400050de70 pc=0xaaaad4bb97dc
sync.(*WaitGroup).Wait(0x40004ca8d8)
	sync/waitgroup.go:118 +0x70 fp=0x400050ded0 sp=0x400050deb0 pc=0xaaaad4bcb3f0
github.com/ollama/ollama/runner/llamarunner.(*Server).run(0x40004ca8c0, {0xaaaad60ac480, 0x4000379db0})
	github.com/ollama/ollama/runner/llamarunner/runner.go:360 +0x40 fp=0x400050dfa0 sp=0x400050ded0 pc=0xaaaad5012bc0
github.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x30 fp=0x400050dfd0 sp=0x400050dfa0 pc=0xaaaad50171f0
runtime.goexit({})
	runtime/asm_arm64.s:1223 +0x4 fp=0x400050dfd0 sp=0x400050dfd0 pc=0xaaaad4bbfd14
created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1
	github.com/ollama/ollama/runner/llamarunner/runner.go:981 +0x44c

r0      0x0
r1      0x2983
r2      0x6
r3      0xffff4595f0c0
r4      0xffff959c4b58
r5      0xffff4595d7f0
r6      0xfffffff8
r7      0xffff4595d7d0
r8      0x83
r9      0x0
r10     0xffff95424860
r11     0x0
r12     0x746977735f747874
r13     0x0
r14     0x3120363131203430
r15     0x3120353331203032
r16     0x1
r17     0xaaaad68f0988
r18     0x1
r19     0x2983
r20     0xffff4595f0c0
r21     0x6
r22     0x1c
r23     0xaaaad5d52954
r24     0xaaaad5d4d981
r25     0xffff4595e248
r26     0x1a40
r27     0xffff3b264380
r28     0xaaaad5d52968
r29     0xffff4595d6e0
lr      0xffff954a1ff4
sp      0xffff4595d650
pc      0xffff954a2008
fault   0x0
time=2026-02-15T12:55:11.353-05:00 level=INFO source=server.go:1384 msg="waiting for server to become available" status="llm server error"
time=2026-02-15T12:55:11.481-05:00 level=ERROR source=server.go:304 msg="llama runner terminated" error="exit status 2"
time=2026-02-15T12:55:11.604-05:00 level=INFO source=sched.go:490 msg="Load failed" model=/home/rameshthiyagu/.ollama/models/blobs/sha256-e554c6b9de016673fd2c732e0342967727e9659ca5f853a4947cc96263fa602b error="llama runner process has terminated: GGML_ASSERT(buffer) failed"
[GIN] 2026/02/15 - 12:55:11 | 500 |  14.22524443s |       127.0.0.1 | POST     "/api/generate"
